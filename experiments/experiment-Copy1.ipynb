{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b61f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib \n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86a6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"./Models\")\n",
    "import os\n",
    "os.system('')\n",
    "\n",
    "from Models.tsrnn import TSRNN, DPTrainableTSRNN\n",
    "from Models.BigBirdSparse.SparseTransformerBB import TransformerBBSparse, TransformerBBFixed\n",
    "from Models.transformer_base import Transformer_Base, DPTrainable\n",
    "from Models.NT.NoAttention import NoAttention\n",
    "# from Models.tsrnn_vit import TSRNN_vit\n",
    "\n",
    "from Datasets.LondonSmartMeter.lsm_def import LondonSmartMeter\n",
    "from Datasets.PJM_energy_datasets.aep_def import AEP #PJM AEP\n",
    "from Datasets.PJM_energy_datasets.dayton_def import DAYTON\n",
    "from Datasets.Spain_EW.spain_def import REE #Spain\n",
    "\n",
    "import copy\n",
    "import random\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Takes in calling python file arguments\n",
    "args = sys.argv\n",
    "\n",
    "# Set to empty list\n",
    "args = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56eed3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default settings for experiment\n",
    "arg_model = \"trfbf\" #Options: 'trfbb', 'tsrnn', 'trfbf'\n",
    "arg_dset = \"dyt\" #Datasets -- Spain: 'ree', AEP: 'aep', DAYTON: 'dyt' London: 'lsm'\n",
    "\n",
    "attr_dset_smpl_rt = 24 if arg_dset == \"aep\" else (48 if arg_dset == \"lsm\" else 24) #Samples per day. Spain, AEP: 24, London: 48\n",
    "param_dset_lookback_weeks = 5\n",
    "# param_dset_forecast = 48 if arg_dset == \"lsm\" else 24\n",
    "# param_dset_lookback_weeks = 9\n",
    "param_dset_forecast = 168 if arg_dset == \"lsm\" else 84 # 3.5days = 168\n",
    "param_dset_train_stride = 48 #Choose a coprime value to the forecast so all reading frames are eventually considered\n",
    "param_dset_test_stride = 'same' #tsrnn paper uses 1 week\n",
    "param_dset_lookback = param_dset_lookback_weeks*7*attr_dset_smpl_rt - param_dset_forecast\n",
    "\n",
    "opt_cudadevcs = None\n",
    "\n",
    "#Transformer only parameters\n",
    "param_trf_edim = 24\n",
    "param_trf_heads = 4\n",
    "param_trf_elyr = 4\n",
    "param_trf_dlyr = 4\n",
    "param_trf_ffdim = 256\n",
    "param_trf_weather = False\n",
    "\n",
    "#Bigbird only parameters\n",
    "param_trf_bksz = 48\n",
    "\n",
    "#Training Params\n",
    "arg_ini_lr = 1e-3\n",
    "# arg_batchsz = 'auto'\n",
    "arg_batchsz = 32\n",
    "arg_epochs = 1000\n",
    "\n",
    "#Output settings\n",
    "arg_outdir = \"Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad8b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read arguments\n",
    "if (len(args) != 1):\n",
    "    args = args[1:] #exclude first argument which is the file name\n",
    "    #Assert that the following values come in argument-value pairs\n",
    "    assert (len(args)%2 == 0), \"Argument-option mismatch\"\n",
    "    \n",
    "    while (len(args) >= 2):\n",
    "        agn = args[0]; agv = args[1]\n",
    "        \n",
    "        if agn == '--model':\n",
    "            assert agv in ['trfbb','tsrnn', 'trfbf']\n",
    "            arg_model = agv\n",
    "        #Dataset related options\n",
    "        elif agn == '--dset':\n",
    "            assert agv in ['aep','lsm','ree', 'dyt']\n",
    "            arg_dset = agv\n",
    "        elif agn == '--lkbckwk':\n",
    "            param_dset_lookback_weeks = int(agv)\n",
    "        elif agn == '--frcst':\n",
    "            param_dset_forecast = int(agv)\n",
    "        elif agn == '--dset-train-stride':\n",
    "            if agv == 'same':\n",
    "                param_dset_train_stride = 'same'\n",
    "            else:\n",
    "                param_dset_train_stride = int(agv)\n",
    "        elif agn == '--dset-test-stride':\n",
    "            if agv == 'same':\n",
    "                param_dset_test_stride = 'same'\n",
    "            else:\n",
    "                param_dset_test_stride = int(agv)\n",
    "        \n",
    "        #CUDA options\n",
    "        elif agn == '--devices':\n",
    "            dev_num = agv.split(',')\n",
    "            for i in range(len(dev_num)):\n",
    "                dev_num[i] = int(dev_num[i])\n",
    "            opt_cudadevcs = dev_num\n",
    "        \n",
    "        #No parameters available for adjusting tsrnn\n",
    "        #Transformer parameters\n",
    "        elif agn == '--trf-edim':\n",
    "            param_trf_edim = int(agv)\n",
    "        elif agn == '--trf-heads':\n",
    "            param_trf_heads = int(agv)\n",
    "        elif agn == '--trf-elyr':\n",
    "            param_trf_elyr = int(agv)\n",
    "        elif agn == '--trf-dlyr':\n",
    "            param_trf_dlyr = int(agv)\n",
    "        elif agn == '--trf-ffdm':\n",
    "            param_trf_ffdim = int(agv)\n",
    "        \n",
    "        #Transformer specific option: include weather data\n",
    "        elif agn == '--weather':\n",
    "            assert agv in ['True','False']\n",
    "            if agv == 'True':\n",
    "                param_trf_weather = True\n",
    "            \n",
    "        #Training parameters\n",
    "        elif agn == '--init-lr':\n",
    "            arg_ini_lr = float(agv)\n",
    "        elif agn == '--bchsz':\n",
    "            if agv == 'auto':\n",
    "                arg_batchsz = \"auto\"\n",
    "            else:\n",
    "                arg_batchsz = int(agv)\n",
    "        elif agn == \"--epochs\":\n",
    "            arg_epochs = int(agv)\n",
    "        #Output parameters\n",
    "        elif agn == '--odir':\n",
    "            arg_outdir = agv\n",
    "        else:\n",
    "            raise ValueError(\"Unknown argument\")\n",
    "        \n",
    "        args = args[2:]\n",
    "\n",
    "#Compute remaining settings\n",
    "param_dset_lookback = param_dset_lookback_weeks*7*attr_dset_smpl_rt - param_dset_forecast\n",
    "if param_dset_train_stride == 'same': param_dset_train_stride = param_dset_forecast\n",
    "if param_dset_test_stride == 'same': param_dset_test_stride = param_dset_forecast\n",
    "attr_dset_smpl_rt = {'ree':24,'aep':24,'lsm':48, 'dyt':24}[arg_dset]\n",
    "\n",
    "param_trf_inp_dim = {'ree':7,'lsm': 14}[arg_dset] if param_trf_weather else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f735960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute BBSparse blocksize iteratively\n",
    "param_trf_bksz = 48\n",
    "nl_bsz = param_trf_bksz\n",
    "nu_bsz = param_trf_bksz\n",
    "\n",
    "while ((param_dset_lookback%nl_bsz) != 0) and ((param_dset_lookback%nu_bsz) != 0):\n",
    "    nl_bsz = nl_bsz - 1\n",
    "    nu_bsz = nu_bsz + 1\n",
    "\n",
    "if (param_dset_lookback % nl_bsz) == 0:\n",
    "    param_trf_bksz = nl_bsz\n",
    "else:\n",
    "    param_trf_bksz = nu_bsz\n",
    "\n",
    "assert (param_trf_bksz >= 24), \"Computed block size too small\"\n",
    "assert (param_trf_bksz <= 64), \"Computed block size too large\"\n",
    "if arg_model == 'trfbb':\n",
    "    print(\"BigBird block size autoset to: \" + str(param_trf_bksz))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b43368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerBBFixed(\n",
       "  (input_linear): Linear(in_features=1, out_features=3, bias=True)\n",
       "  (pe): LearnablePositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (trf_el): TransformerEncoderLayer(\n",
       "    (self_attn): BlockSparseFixedAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (norm1): TBatchNorm(\n",
       "      (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (norm2): TBatchNorm(\n",
       "      (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (trf_dl): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (norm1): TBatchNorm(\n",
       "      (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (norm2): TBatchNorm(\n",
       "      (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (norm3): TBatchNorm(\n",
       "      (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (trf_e): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): BlockSparseFixedAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "        (norm1): TBatchNorm(\n",
       "          (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (norm2): TBatchNorm(\n",
       "          (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trf_d): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=3, out_features=3, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "        (norm1): TBatchNorm(\n",
       "          (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (norm2): TBatchNorm(\n",
       "          (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (norm3): TBatchNorm(\n",
       "          (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (aux_out): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (aux_in): Linear(in_features=1, out_features=3, bias=True)\n",
       "  (expand): Linear(in_features=756, out_features=5120, bias=True)\n",
       "  (compress): Linear(in_features=5120, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup experiment\n",
    "#Seed RNG\n",
    "seed = (time.time_ns()//1000) % 1000000\n",
    "torch.random.manual_seed(seed) #126982\n",
    "\n",
    "#Setup Model\n",
    "model = None\n",
    "dpm = None\n",
    "if arg_model == \"trfbb\":\n",
    "    model = TransformerBBSparse(seq_len = param_dset_lookback,\n",
    "                                out_seq_len = param_dset_forecast,\n",
    "                                inp_dim = param_trf_inp_dim,\n",
    "                                emb_dim = param_trf_edim,\n",
    "                                n_heads = param_trf_heads,\n",
    "                                n_enc_layers = param_trf_elyr,\n",
    "                                n_dec_layers = param_trf_dlyr,\n",
    "                                block_size=param_trf_bksz,\n",
    "                                ffdim = param_trf_ffdim)\n",
    "    dpm = DPTrainable(model)\n",
    "    \n",
    "elif arg_model == 'tsrnn':\n",
    "    model = TSRNN(smpl_rate = attr_dset_smpl_rt,\n",
    "                  pred_horz = param_dset_forecast,\n",
    "                  num_weeks = param_dset_lookback_weeks)\n",
    "    dpm = DPTrainableTSRNN(model,cuda_devices=opt_cudadevcs)\n",
    "\n",
    "elif arg_model == 'trfbf':\n",
    "    model = TransformerBBFixed(seq_len = param_dset_lookback,\n",
    "                                out_seq_len = param_dset_forecast,\n",
    "                                inp_dim = param_trf_inp_dim,\n",
    "                                # emb_dim = param_trf_edim,\n",
    "                                emb_dim = param_trf_inp_dim * 3,\n",
    "                                # n_heads = param_trf_heads,\n",
    "                                n_heads = 3,\n",
    "                                n_enc_layers = param_trf_elyr,\n",
    "                                n_dec_layers = param_trf_dlyr,\n",
    "                                block_size=param_trf_bksz,\n",
    "                                ffdim = param_trf_ffdim)\n",
    "    dpm = DPTrainable(model)\n",
    "\n",
    "# cuda_lead = torch.device(\"cuda\",opt_cudadevcs[0]) if type(opt_cudadevcs) is list else torch.device(\"cuda:0\")\n",
    "cuda_lead = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(cuda_lead)\n",
    "model.to(cuda_lead)\n",
    "#Try to JIT script model\n",
    "# jit_test = torch.jit.script(model)\n",
    "# #print(\"Model structure:\\n\",jit_test)\n",
    "# del(jit_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df3d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatic batchsize\n",
    "num_active_devices = len(opt_cudadevcs) if type(opt_cudadevcs) is list else torch.cuda.device_count()\n",
    "if arg_batchsz == 'auto':\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    #Dummy passes to measure memory use\n",
    "    max_mem = 0\n",
    "    num_batches = 0\n",
    "    test_limit = 1000\n",
    "    for step in tqdm([10,3,1]):\n",
    "        for num_batches in tqdm(range(max(num_batches,step),test_limit,step)):\n",
    "            try:\n",
    "                x = None\n",
    "                im_l = None\n",
    "                # with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                    if arg_model in ['tsrnn','tsvit']:\n",
    "                        x, im_l = model(torch.zeros((num_batches*16,param_dset_lookback,param_trf_inp_dim)).to(cuda_lead),\n",
    "                                  torch.zeros((num_batches*16,param_dset_forecast,param_trf_inp_dim)).to(cuda_lead))\n",
    "                        torch.cuda.amp.GradScaler().scale((x*x).sum() + im_l).backward(retain_graph = True)\n",
    "                    else:\n",
    "                        x = model(torch.zeros((num_batches*16,param_dset_lookback,param_trf_inp_dim)).to(cuda_lead))\n",
    "                        torch.cuda.amp.GradScaler().scale((x*x).sum()).backward(retain_graph = True)\n",
    "                max_mem = torch.cuda.max_memory_allocated(cuda_lead)\n",
    "            except RuntimeError as err: #trap the runtime OOM error\n",
    "                if 'CUDA out of memory' not in str(err):\n",
    "                    raise err\n",
    "                test_limit = num_batches\n",
    "                num_batches -= step\n",
    "                break\n",
    "            finally:\n",
    "                if arg_model in ['tsrnn','tsvit']:\n",
    "                    del(im_l)\n",
    "                del(x)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    avail_mem = 0\n",
    "    if type(opt_cudadevcs) is list:\n",
    "        for dev_num in opt_cudadevcs:\n",
    "            avail_mem += torch.cuda.mem_get_info(torch.cuda.device(dev_num))[0]\n",
    "    else:\n",
    "        for dev in range(torch.cuda.device_count()):\n",
    "            avail_mem += torch.cuda.mem_get_info(torch.cuda.device(dev))[0]\n",
    "    auto_bsz = (int(0.87*num_batches)*16*num_active_devices) #int((0.85*avail_mem)//max_mem)*16*num_active_devices\n",
    "    arg_batchsz = auto_bsz\n",
    "    print(\"Max memory allocated: \", max_mem)\n",
    "    print(\"Current available memory: \",avail_mem)\n",
    "    print(\"Automatic batch size selection: {0} ({1})\".format(arg_batchsz,auto_bsz//16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4628b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Dataset\n",
    "train_set = val_set = test_set = None\n",
    "if arg_dset == 'aep':\n",
    "    train_offset = 0\n",
    "    val_offset = 9 \n",
    "    #with the definitions for split boundaries, this offset for the start of the validation set ensures the start time matches the train and test sets.\n",
    "    train_set = AEP(path = \"Datasets/PJM_energy_datasets\",\n",
    "                    start_idx = train_offset, end_idx = (4042*24)+param_dset_forecast - 12129,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=param_dset_train_stride,\n",
    "                    timestamp = False)\n",
    "    val_set = AEP(path = \"Datasets/PJM_energy_datasets\",\n",
    "                  start_idx = val_offset + (4042*24)+param_dset_forecast - 12129,\n",
    "                  end_idx = (4042*24)+param_dset_forecast,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=param_dset_train_stride,\n",
    "                    timestamp = False)\n",
    "    test_set = AEP(path = \"Datasets/PJM_energy_datasets\",\n",
    "                    start_idx = (4042*24)+param_dset_forecast, #~Last 20% of dataset\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=param_dset_test_stride,\n",
    "                    timestamp = False)\n",
    "\n",
    "    #Monkey patch the dataset to normalize the series\n",
    "    train_set.series = (train_set.series - 9581)/(25695 - 9581)\n",
    "    val_set.series = (val_set.series - 9581)/(25695 - 9581)\n",
    "    test_set.series = (test_set.series - 9581)/(25695 - 9581)\n",
    "\n",
    "elif arg_dset == 'dyt':\n",
    "    full_set = DAYTON(path = \"Datasets/PJM_energy_datasets\",\n",
    "                      seq_len = param_dset_lookback,\n",
    "                      pred_horz = param_dset_forecast,\n",
    "                      timestamp = False)\n",
    "    dytmax = full_set.max()\n",
    "    dytmin = full_set.min()\n",
    "    del(full_set)\n",
    "    \n",
    "    train_set = DAYTON(path = \"Datasets/PJM_energy_datasets\",\n",
    "                    start_idx = 0, end_idx = 97036,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=29,\n",
    "                    timestamp = False)\n",
    "    val_set = DAYTON(path = \"Datasets/PJM_energy_datasets\",\n",
    "                  start_idx = 97036, end_idx = 97036+12129,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=param_dset_forecast,\n",
    "                    timestamp = False)\n",
    "    test_set = DAYTON(path = \"Datasets/PJM_energy_datasets\",\n",
    "                    start_idx = 97036+12129,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=param_dset_forecast,\n",
    "                    timestamp = False)\n",
    "    \n",
    "    train_set.series = (train_set.series - dytmin)/(dytmax - dytmin)\n",
    "    val_set.series = (val_set.series - dytmin)/(dytmax - dytmin)\n",
    "    test_set.series = (test_set.series - dytmin)/(dytmax - dytmin)\n",
    "\n",
    "elif arg_dset == 'ree':\n",
    "    train_set = REE(path = \"Datasets/Spain_EW\",\n",
    "                    start_idx = 0, end_idx = 28051,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=7,\n",
    "                    timestamp = False, weather = param_trf_weather)\n",
    "    val_set = REE(path = \"Datasets/Spain_EW\",\n",
    "                  start_idx = 28051, end_idx = 28051+3506,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=19,\n",
    "                    timestamp = False, weather = param_trf_weather)\n",
    "    test_set = REE(path = \"Datasets/Spain_EW\",\n",
    "                    start_idx = 28051+3506,\n",
    "                    seq_len = param_dset_lookback,\n",
    "                    pred_horz = param_dset_forecast,\n",
    "                    stride=19,\n",
    "                    timestamp = False, weather = param_trf_weather)\n",
    "    \n",
    "    reemin = train_set.min()\n",
    "    reemax = train_set.max()\n",
    "    \n",
    "    #Monkey patch dataset to normalize series\n",
    "    train_set.series = (train_set.series - reemin)/(reemax - reemin)\n",
    "    val_set.series = (val_set.series - reemin)/(reemax - reemin)\n",
    "    test_set.series = (test_set.series - reemin)/(reemax - reemin)\n",
    "    \n",
    "elif arg_dset == 'lsm':\n",
    "    \n",
    "    # Create class\n",
    "    dset = LondonSmartMeter(path='Datasets/LondonSmartMeter',\n",
    "                            seq_len=param_dset_lookback,\n",
    "                            pred_horz=param_dset_forecast,weather=param_trf_weather)\n",
    "    \n",
    "    # Use __getitem__ method in class\n",
    "    # print(dset[0])\n",
    "    \n",
    "    h_idcs = dset.get_household_indices()\n",
    "    #h_idcs = [(hno,idcs) for hno, idcs in enumerate(h_idcs)]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(h_idcs)\n",
    "    \n",
    "    # train_idcs, val_idcs, test_idcs =\\\n",
    "    #     h_idcs[:3*len(h_idcs)//5],\\\n",
    "    #     h_idcs[3*len(h_idcs)//5:4*len(h_idcs)//5],\\\n",
    "    #     h_idcs[4*len(h_idcs)//5:]\n",
    "\n",
    "    train_idcs, val_idcs, test_idcs =\\\n",
    "        h_idcs[:200],\\\n",
    "        h_idcs[200:300],\\\n",
    "        h_idcs[300:400]\n",
    "    \n",
    "    \n",
    "    train_idcs = [ idx for h in train_idcs for idx in h ]\n",
    "    val_idcs = [ idx for h in val_idcs for idx in h ]\n",
    "    test_idcs = [ idx for h in test_idcs for idx in h ]\n",
    "\n",
    "\n",
    "    train_set, val_set = torch.utils.data.Subset(dset, train_idcs),\\\n",
    "                            torch.utils.data.Subset(dset, val_idcs)\n",
    "    \n",
    "    test_set = torch.utils.data.Subset(dset, test_idcs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc4cd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual number of model parameters: 4326992\n",
      "Trainable model parameters: 4326992\n",
      "2\n",
      "32\n",
      "756\n",
      "84\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Setup dataloaders\n",
    "custom_collate = torch.utils.data.default_collate\n",
    "if param_trf_weather:\n",
    "    if (arg_dset == 'ree'):\n",
    "        custom_collate = lambda dat: torch.utils.data.default_collate(\n",
    "            [(torch.cat((elem[0],elem[1]),dim=-1),elem[2]) for elem in dat])\n",
    "    # elif (arg_dset == 'lsm'):\n",
    "    #     custom_collate = lambda dat: torch.utils.data.default_collate(\n",
    "    #         [(elem[0],elem[1][:,0]) for elem in dat])\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=arg_batchsz,\n",
    "                                           shuffle=True, collate_fn= custom_collate,\n",
    "                                           pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set,batch_size=arg_batchsz, collate_fn = custom_collate,pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,batch_size=arg_batchsz, collate_fn = custom_collate)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(),lr = arg_ini_lr)\n",
    "scheduler= None\n",
    "if arg_model == \"tsrnn\":\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda epoch: 1)\n",
    "\n",
    "#Transformers\n",
    "else:\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt,\n",
    "                                        lr_lambda = lambda epoch: 1 if epoch < 2*arg_epochs//6 else\n",
    "                                                                  0.1 if epoch < 3*arg_epochs//6 else\n",
    "                                                                  0.03 if epoch < 4*arg_epochs//6 else\n",
    "                                                                  0.01)\n",
    "\n",
    "# Print model summary\n",
    "# print(model)\n",
    "def num_parameters(m):\n",
    "    return sum([p.numel() for p in m.parameters()])\n",
    "\n",
    "parameters = num_parameters(model)\n",
    "\n",
    "# print(f\"Expected number of parameters: {m * dk * dk + m * 1 * 1 * n}\")\n",
    "print(f\"Actual number of model parameters: {parameters}\")\n",
    "\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"Trainable model parameters: {trainable_params}\" )    \n",
    "\n",
    "# print(next(iter(train_loader)))\n",
    "print(len(next(iter(train_loader))))\n",
    "print(len(next(iter(train_loader))[0]))\n",
    "print(len(next(iter(train_loader))[0][0]))  # Lookback 1632 for lsm\n",
    "print(len(next(iter(train_loader))[1][0]))  # Prediction for next 48 for lsm\n",
    "print(len(next(iter(train_loader))[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22f71068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f4390d76c84dfd8f05467a05630901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.049403524545666114, val_loss =\u001b[1;32m 0.12160271406173706 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 1: loss = 0.021848411335108373, val_loss = 0.1231946274638176\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 2: loss = 0.0201965582360012, val_loss = 0.12312421202659607\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 3: loss = 0.019870867307942647, val_loss = 0.12453819811344147\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 4: loss = 0.019391591114421878, val_loss = 0.12368825078010559\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 5: loss = 0.019023795916627232, val_loss = 0.121624656021595\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 6: loss = 0.01922882204123128, val_loss = 0.12345875054597855\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 7: loss = 0.018905548907171648, val_loss = 0.12288215011358261\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 8: loss = 0.01899784560709332, val_loss = 0.12601499259471893\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 9: loss = 0.018564992512647923, val_loss = 0.12755393981933594\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 10: loss = 0.018219504021824554, val_loss = 0.12449812889099121\n",
      "\u001b[1;36mBest validation loss: 0.12160271406173706\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 11: loss = 0.01695087509086499, val_loss =\u001b[1;32m 0.11952278763055801 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.11952278763055801\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 12: loss = 0.01672258528056913, val_loss =\u001b[1;32m 0.11197519302368164 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.11197519302368164\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 13: loss = 0.016279597068205476, val_loss =\u001b[1;32m 0.11067544668912888 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.11067544668912888\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 14: loss = 0.012143459466572564, val_loss =\u001b[1;32m 0.0886804386973381 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.0886804386973381\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 15: loss = 0.009310561160628613, val_loss =\u001b[1;32m 0.0828687846660614 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.0828687846660614\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 16: loss = 0.008087782301187802, val_loss =\u001b[1;32m 0.07980134338140488 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.07980134338140488\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 17: loss = 0.007910053411391206, val_loss =\u001b[1;32m 0.07812857627868652 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.07812857627868652\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 18: loss = 0.007738403604330065, val_loss = 0.07918964326381683\n",
      "\u001b[1;36mBest validation loss: 0.07812857627868652\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 19: loss = 0.0074941845870433515, val_loss =\u001b[1;32m 0.07812418043613434 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 20: loss = 0.0074274585170384785, val_loss = 0.08696859329938889\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 21: loss = 0.007226328011781264, val_loss = 0.09082948416471481\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 22: loss = 0.0071135101830945, val_loss = 0.08199316263198853\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 23: loss = 0.006712587832365758, val_loss = 0.0886714830994606\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 24: loss = 0.006606035198693952, val_loss = 0.08987080305814743\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 25: loss = 0.006683528799420366, val_loss = 0.0805080384016037\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 26: loss = 0.006353054392653016, val_loss = 0.0957261472940445\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 27: loss = 0.006321401523354535, val_loss = 0.09084766358137131\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 28: loss = 0.006165412962078475, val_loss = 0.09447138756513596\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 29: loss = 0.006115833713780516, val_loss = 0.08435980975627899\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.12265227735042572\u001b[1;37m\n",
      "Epoch 30: loss = 0.006045292950987529, val_loss = 0.08642175048589706\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 31: loss = 0.00594504350850072, val_loss = 0.08664675056934357\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 32: loss = 0.005673847432122924, val_loss = 0.08650712668895721\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 33: loss = 0.005795634483757357, val_loss = 0.08606120198965073\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 34: loss = 0.005802726608145839, val_loss = 0.09505673497915268\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 35: loss = 0.005555835926385883, val_loss = 0.07932053506374359\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 36: loss = 0.00532985705201729, val_loss = 0.08503079414367676\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 37: loss = 0.0052433605261075385, val_loss = 0.09777532517910004\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 38: loss = 0.005280860274349554, val_loss = 0.09491154551506042\n",
      "\u001b[1;36mBest validation loss: 0.07812418043613434\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 39: loss = 0.00519621616247325, val_loss =\u001b[1;32m 0.06520533561706543 \u001b[1;37m\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 40: loss = 0.004927658037139246, val_loss = 0.07978115230798721\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 41: loss = 0.004943547667398189, val_loss = 0.07702115178108215\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 42: loss = 0.004869035306350829, val_loss = 0.07056968659162521\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 43: loss = 0.004663609093628251, val_loss = 0.07504928112030029\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 44: loss = 0.004699248184736531, val_loss = 0.07272326201200485\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 45: loss = 0.0044665135725293886, val_loss = 0.07653794437646866\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 46: loss = 0.0044971192031740565, val_loss = 0.07165688276290894\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 47: loss = 0.004166524866237664, val_loss = 0.06778652220964432\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 48: loss = 0.004219200661352191, val_loss = 0.07860676944255829\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 49: loss = 0.0041179129329975694, val_loss = 0.07554281502962112\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 50: loss = 0.004042870826266993, val_loss = 0.07919960469007492\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 51: loss = 0.0038642026819826034, val_loss = 0.09847801923751831\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 52: loss = 0.0037624589525736296, val_loss = 0.0679149255156517\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 53: loss = 0.0036229419727953007, val_loss = 0.07649629563093185\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 54: loss = 0.0035488344505071067, val_loss = 0.07320653647184372\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 55: loss = 0.0035650054498826368, val_loss = 0.0743195191025734\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 56: loss = 0.0035744117541561047, val_loss = 0.09070207178592682\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 57: loss = 0.0033164591116544148, val_loss = 0.06912877410650253\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 58: loss = 0.0034641424807397504, val_loss = 0.07280570268630981\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 59: loss = 0.0033284342565681213, val_loss = 0.07143393158912659\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.08295319229364395\u001b[1;37m\n",
      "Epoch 60: loss = 0.003141877883955693, val_loss = 0.06859869509935379\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 61: loss = 0.002999519801680715, val_loss = 0.07923544943332672\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 62: loss = 0.0030608712538826065, val_loss = 0.079172283411026\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 63: loss = 0.0028537487579831998, val_loss = 0.08753933012485504\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 64: loss = 0.002769208212087576, val_loss = 0.07835239917039871\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 65: loss = 0.002821927479369781, val_loss = 0.07081443071365356\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 66: loss = 0.0029321701843470624, val_loss = 0.07546021044254303\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 67: loss = 0.0027197344517424847, val_loss = 0.07834110409021378\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 68: loss = 0.002691562729663789, val_loss = 0.07304803282022476\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 69: loss = 0.0026476102659496693, val_loss = 0.07454799115657806\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 70: loss = 0.002670090780996431, val_loss = 0.07178867608308792\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 71: loss = 0.002481701286832014, val_loss = 0.06967968493700027\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 72: loss = 0.002455568239943554, val_loss = 0.08331924676895142\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 73: loss = 0.0024639152890393655, val_loss = 0.07473687827587128\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 74: loss = 0.0023550485180189405, val_loss = 0.09310884028673172\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 75: loss = 0.0023777090368989427, val_loss = 0.08063607662916183\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 76: loss = 0.002460598447494424, val_loss = 0.07899454236030579\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 77: loss = 0.002318570711605179, val_loss = 0.07619983702898026\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 78: loss = 0.0022990480401159194, val_loss = 0.07775519788265228\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 79: loss = 0.002149752138603407, val_loss = 0.0760645717382431\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 80: loss = 0.0021107544979223837, val_loss = 0.0858394205570221\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 81: loss = 0.002144238574636312, val_loss = 0.06910017877817154\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 82: loss = 0.0020450460153649775, val_loss = 0.07519745826721191\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 83: loss = 0.0020370432522703106, val_loss = 0.08685209602117538\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 84: loss = 0.0019728359408103502, val_loss = 0.07784170657396317\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 85: loss = 0.0019511473230802668, val_loss = 0.08089052885770798\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 86: loss = 0.002012927784772518, val_loss = 0.08751305192708969\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 87: loss = 0.0019083598879380869, val_loss = 0.07664729654788971\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 88: loss = 0.0018835969027489997, val_loss = 0.07689278572797775\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 89: loss = 0.0017999257845356343, val_loss = 0.07622930407524109\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 90: loss = 0.001940790307484209, val_loss = 0.08457592129707336\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 91: loss = 0.0018911191179470804, val_loss = 0.0799965187907219\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 92: loss = 0.0019318791086194464, val_loss = 0.0811544880270958\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 93: loss = 0.0017956344907780965, val_loss = 0.07326330244541168\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 94: loss = 0.0017727118728754038, val_loss = 0.08359760791063309\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 95: loss = 0.0017319122125627473, val_loss = 0.08296895772218704\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 96: loss = 0.0016932980126092355, val_loss = 0.08301438391208649\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 97: loss = 0.001607418848344913, val_loss = 0.0848013162612915\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 98: loss = 0.0016621807668483458, val_loss = 0.07635042816400528\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Epoch 99: loss = 0.0017139619258859267, val_loss = 0.07589410245418549\n",
      "\u001b[1;36mBest validation loss: 0.06520533561706543\u001b[1;37m\n",
      "\u001b[1;94mCurrent test loss (Updated every 30 epochs): 0.0697750523686409\u001b[1;37m\n",
      "Estimated time to complete: 0h 0m"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_model_state = None\n",
    "best_model_dpt = copy.deepcopy(dpm)\n",
    "best_val = 999999\n",
    "current_test = 999999\n",
    "\n",
    "epoch_start = 0\n",
    "epoch_end = 0\n",
    "avg_epoch_dur = 0\n",
    "\n",
    "arg_epochs = 100\n",
    "\n",
    "for i in tqdm(range(arg_epochs), total=len(range(arg_epochs)), desc=\"epochs\"):\n",
    "    epoch_start = time.time_ns()\n",
    "    \n",
    "    l,ls = dpm.train_epoch(train_loader, opt, device = cuda_lead, scaler = scaler)    \n",
    "    vl,vls = dpm.val(val_loader,\n",
    "                     loss_fn = lambda x,y: torch.nn.MSELoss(reduction='none')(x, y).\\\n",
    "                             nanmean(dim=-2).\\\n",
    "                             sqrt_(),\n",
    "                    device = cuda_lead)\n",
    "    \n",
    "    epoch_end = time.time_ns()\n",
    "    vl_improved = (vl < best_val)\n",
    "    if vl_improved:\n",
    "        best_val = vl\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if (i%30 == 0):\n",
    "        best_model_dpt.module.load_state_dict(best_model_state)\n",
    "        current_test,_ = best_model_dpt.val(test_loader,lambda x,y: torch.nn.MSELoss(reduction='none')(x, y).\\\n",
    "                                      nanmean(dim=-2).sqrt_(),device=cuda_lead)\n",
    "        del(_)\n",
    "    #print(\"\\r                                                     \",end=\"\\r\")\n",
    "    print(\"\\033[2K\\033[1A\\033[2K\\033[1A\\033[2K\",end=\"\\r\")\n",
    "    if vl_improved:\n",
    "        print(\"Epoch {0}: loss = {1}, val_loss =\\033[1;32m {2} \\033[1;37m\".format(i,l,vl))\n",
    "    else:\n",
    "        print(\"Epoch {0}: loss = {1}, val_loss = {2}\".format(i,l,vl))\n",
    "    \n",
    "    #Estimate completion time\n",
    "    epoch_dur_sec = (epoch_end-epoch_start)/(1000000000)\n",
    "    if i == 0:\n",
    "        avg_epoch_dur = epoch_dur_sec\n",
    "    avg_epoch_dur = 0.3*epoch_dur_sec + 0.7*avg_epoch_dur\n",
    "    etc_sec = (arg_epochs - i - 1)*avg_epoch_dur\n",
    "    est_hrs = int(etc_sec//3600)\n",
    "    est_mins = round((etc_sec%3600)/60)\n",
    "    if est_mins == 60:\n",
    "        est_mins = 0\n",
    "        est_hrs += 1\n",
    "    print(\"\\033[1;36mBest validation loss: {0}\\033[1;37m\".format(best_val))\n",
    "    print(\"\\033[1;94mCurrent test loss (Updated every 30 epochs): {0}\\033[1;37m\".format(current_test))\n",
    "    print(\"Estimated time to complete: {0}h {1}m\".format(est_hrs,est_mins),end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe28bd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Test loss (Final Epoch): [0.007157455664128065, 0.07568656653165817, 0.06393483281135559, 0.1834096759557724]\n",
      "Test loss (Best Validation): [0.005813730414956808, 0.0697750523686409, 0.05929718539118767, 0.16590049862861633]\n"
     ]
    },
    {
     "ename": "NotSupportedError",
     "evalue": "Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n  File \"C:\\Users\\linwa\\OneDrive\\Desktop\\FYP_SWT\\experiments\\Models\\BigBirdSparse\\bb_fixed.py\", line 470\n    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n                need_weights: bool = True, attn_mask: Optional[Tensor] = None, **kwargs) -> Tuple[Tensor, Optional[Tensor]]:\n                                                                                ~~~~~~~ <--- HERE\n        if self.batch_first:\n            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(arg_outdir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m     ofile\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeed, MSE, RMSE, MAE, sMAPE, Trained Model Filename\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m scripted_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_dpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m ofname \u001b[38;5;241m=\u001b[39m arg_model \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(param_dset_forecast) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m arg_dset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(seed) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.smd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m scripted_model\u001b[38;5;241m.\u001b[39msave(arg_outdir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ofname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_script.py:1284\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m   1283\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[1;32m-> 1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:480\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[0;32m    479\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[1;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[0;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[1;34m(cpp_module, init_fn)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[1;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[0;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[1;34m(script_module)\u001b[0m\n\u001b[0;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[0;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[0;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[1;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[1;34m(cpp_module, init_fn)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[1;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[0;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[1;34m(script_module)\u001b[0m\n\u001b[0;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[0;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:492\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03mConvert an nn.Module to a RecursiveScriptModule.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    stubs_fn:  Lambda that takes an nn.Module and generates a list of ScriptMethodStubs to compile.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    491\u001b[0m cpp_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_create_module_with_type(concrete_type\u001b[38;5;241m.\u001b[39mjit_type)\n\u001b[1;32m--> 492\u001b[0m method_stubs \u001b[38;5;241m=\u001b[39m \u001b[43mstubs_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m property_stubs \u001b[38;5;241m=\u001b[39m get_property_stubs(nn_module)\n\u001b[0;32m    494\u001b[0m hook_stubs, pre_hook_stubs \u001b[38;5;241m=\u001b[39m get_hook_stubs(nn_module)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:761\u001b[0m, in \u001b[0;36minfer_methods_to_compile\u001b[1;34m(nn_module)\u001b[0m\n\u001b[0;32m    759\u001b[0m stubs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m uniqued_methods:\n\u001b[1;32m--> 761\u001b[0m     stubs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmake_stub_from_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m overload_stubs \u001b[38;5;241m+\u001b[39m stubs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:73\u001b[0m, in \u001b[0;36mmake_stub_from_method\u001b[1;34m(nn_module, method_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Make sure the name present in the resulting AST will match the name\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# requested here. The only time they don't match is if you do something\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# like:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# In this case, the actual function object will have the name `_forward`,\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# even though we requested a stub for `forward`.\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_stub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\_recursive.py:58\u001b[0m, in \u001b[0;36mmake_stub\u001b[1;34m(func, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_stub\u001b[39m(func, name):\n\u001b[0;32m     57\u001b[0m     rcb \u001b[38;5;241m=\u001b[39m _jit_internal\u001b[38;5;241m.\u001b[39mcreateResolutionCallbackFromClosure(func)\n\u001b[1;32m---> 58\u001b[0m     ast \u001b[38;5;241m=\u001b[39m \u001b[43mget_jit_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRecursiveScriptModule\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ScriptMethodStub(rcb, ast, func)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\frontend.py:297\u001b[0m, in \u001b[0;36mget_jit_def\u001b[1;34m(fn, def_name, self_name, is_classmethod)\u001b[0m\n\u001b[0;32m    294\u001b[0m     qualname \u001b[38;5;241m=\u001b[39m get_qualified_name(fn)\n\u001b[0;32m    295\u001b[0m     pdt_arg_types \u001b[38;5;241m=\u001b[39m type_trace_db\u001b[38;5;241m.\u001b[39mget_args_types(qualname)\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdef_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdt_arg_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpdt_arg_types\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\frontend.py:335\u001b[0m, in \u001b[0;36mbuild_def\u001b[1;34m(ctx, py_def, type_line, def_name, self_name, pdt_arg_types)\u001b[0m\n\u001b[0;32m    330\u001b[0m body \u001b[38;5;241m=\u001b[39m py_def\u001b[38;5;241m.\u001b[39mbody\n\u001b[0;32m    331\u001b[0m r \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mmake_range(py_def\u001b[38;5;241m.\u001b[39mlineno,\n\u001b[0;32m    332\u001b[0m                    py_def\u001b[38;5;241m.\u001b[39mcol_offset,\n\u001b[0;32m    333\u001b[0m                    py_def\u001b[38;5;241m.\u001b[39mcol_offset \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 335\u001b[0m param_list \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_param_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpy_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdt_arg_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(py_def, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\torch\\jit\\frontend.py:359\u001b[0m, in \u001b[0;36mbuild_param_list\u001b[1;34m(ctx, py_args, self_name, pdt_arg_types)\u001b[0m\n\u001b[0;32m    357\u001b[0m     expr \u001b[38;5;241m=\u001b[39m py_args\u001b[38;5;241m.\u001b[39mkwarg\n\u001b[0;32m    358\u001b[0m     ctx_range \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mmake_range(expr\u001b[38;5;241m.\u001b[39mlineno, expr\u001b[38;5;241m.\u001b[39mcol_offset \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, expr\u001b[38;5;241m.\u001b[39mcol_offset \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(expr\u001b[38;5;241m.\u001b[39marg))\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotSupportedError(ctx_range, _vararg_kwarg_err)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m py_args\u001b[38;5;241m.\u001b[39mvararg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m     expr \u001b[38;5;241m=\u001b[39m py_args\u001b[38;5;241m.\u001b[39mvararg\n",
      "\u001b[1;31mNotSupportedError\u001b[0m: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n  File \"C:\\Users\\linwa\\OneDrive\\Desktop\\FYP_SWT\\experiments\\Models\\BigBirdSparse\\bb_fixed.py\", line 470\n    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\n                need_weights: bool = True, attn_mask: Optional[Tensor] = None, **kwargs) -> Tuple[Tensor, Optional[Tensor]]:\n                                                                                ~~~~~~~ <--- HERE\n        if self.batch_first:\n            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\")\n",
    "final_model_state = model.state_dict()\n",
    "for key in final_model_state:\n",
    "    final_model_state[key] = final_model_state[key].cpu().detach().numpy()\n",
    "\n",
    "#best_model_dpt = copy.deepcopy(dpm)\n",
    "best_model_dpt.module.load_state_dict(best_model_state)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "losses = [ lambda x,y: torch.nn.MSELoss(reduction='none')(x, y).\\\n",
    "                          nanmean(dim=-2),\n",
    "           lambda x,y: torch.nn.MSELoss(reduction='none')(x, y).\\\n",
    "                                     nanmean(dim=-2).sqrt_(), \n",
    "           lambda x,t: (x-t).abs_().nanmean(dim=-2),\n",
    "           lambda x,t: (2*(t-x).abs_() / (t.abs() + x.abs())).nanmean(dim=-2)]\n",
    "\n",
    "test_loss , tls = dpm.val(test_loader,\n",
    "                          loss_fn = losses,\n",
    "                          device = cuda_lead)\n",
    "print(\"Test loss (Final Epoch): {0}\".format(test_loss))\n",
    "test_loss , tls = best_model_dpt.val(test_loader,\n",
    "                          loss_fn = losses,\n",
    "                          device = cuda_lead)\n",
    "print(\"Test loss (Best Validation): {0}\".format(test_loss))\n",
    "\n",
    "#Write output\n",
    "ofile = None\n",
    "try:\n",
    "    ofile = open(arg_outdir + \"/results.csv\", mode = 'a')\n",
    "except FileNotFoundError:\n",
    "    os.makedirs(arg_outdir)\n",
    "    ofile = open(arg_outdir + \"/results.csv\", mode = 'a')\n",
    "    ofile.write(\"Seed, MSE, RMSE, MAE, sMAPE, Trained Model Filename\\n\")\n",
    "\n",
    "scripted_model = torch.jit.script(best_model_dpt.module)\n",
    "ofname = arg_model + str(param_dset_forecast) + \"_\" + arg_dset + \"_\" + str(seed) + \".smd\"\n",
    "scripted_model.save(arg_outdir + \"/\" + ofname)\n",
    "\n",
    "ofile.write(str(seed) + \",\"\n",
    "            + str(test_loss[0]) + \",\"\n",
    "            + str(test_loss[1]) + \",\"\n",
    "            + str(test_loss[2]) + \",\"\n",
    "            + str(test_loss[3]) + \",\"\n",
    "            + ofname + \"\\n\")\n",
    "\n",
    "ofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a371a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swt_env",
   "language": "python",
   "name": "swt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
