{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b339f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1480565333102413514\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4268752896\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2188142922638490409\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "import tensorflow as tf\n",
    "print(device_lib.list_local_devices())\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86f97af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests require pytest, pytest not installed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "h5py.run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecda64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lyes @2022\n",
    "# The data could be found in http://keddiyan.com/files/PowerForecast.html\n",
    "# and T.-Y. Kim and S.-B. Cho, ‘‘Predicting residential energy consumption using CNN–LSTM neural networks,’’ Energy, vol. 182, pp. 72–81, Sep. 2019.\n",
    "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
    "\n",
    "#https://github.com/LyesSaadSaoud/House_transformer/blob/main/Transformers_Houses1to5_5min.py\n",
    "import sys\n",
    "sys.path.append(\"mypath\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pywt\n",
    "import tensorflow\n",
    "from numpy.random import seed\n",
    "# plt.style.use('seaborn')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fca5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70c46bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'Year': 'Int64',\n",
    "    'Month': 'Int64',\n",
    "    'Day': 'Int64',\n",
    "    'Hour': 'Int64',\n",
    "    'Minute': 'Int64',\n",
    "    'Global_active_power':'float32',\n",
    "    'Global_reactive_power':'float32',\n",
    "    'Voltage;Global_intensity':'float32',\n",
    "    'Sub_metering_1':'float32',\n",
    "    'Sub_metering_2':'float32',\n",
    "    'Sub_metering_3':'float32'\n",
    "}\n",
    "\n",
    "def save_result(y_test,predicted_values):\n",
    "    np.savetxt('./T_SWT_house4_min5_test.csv',y_test) # save path\n",
    "    np.savetxt('./T_SWT_house4_min5_predicted.csv',predicted_values) # save path\n",
    "\n",
    "df=pd.read_csv('C:/Users/Andrew/Desktop/experiments/SWT-Transformer/data ukdale/house1_5mins.csv', dtype=dtype)# path to data\n",
    "\n",
    "def data_preparation(dataset, window, lev):\n",
    "    da = []\n",
    "    for i in range(len(dataset)-window):\n",
    "        coeffs = pywt.swt(dataset[i:window+i], wavelet='db2', level=lev)\n",
    "        da.append(coeffs);\n",
    "    return da\n",
    "\n",
    "def data_reconstruction(dataset,window):\n",
    "    da = []\n",
    "    for i in tqdm(range(len(dataset)), total= len(dataset), desc=\"iswt\"):\n",
    "#         recon = pywt.iswt(dataset[i,:,:,:].tolist(), 'db2')\n",
    "        recon = pywt.iswt(dataset[i], 'db2')\n",
    "#         print(np.array(recon).shape)\n",
    "        da.append(recon[window-1])\n",
    "#         da.append(recon[0][window-1])\n",
    "    return da\n",
    "\n",
    "\n",
    "# Called because iswt cannot accept tolist() dataset\n",
    "def data_organization(coeffs):\n",
    "    '''\n",
    "    Reshape data back to (n,3,2,window_length), where there are 3 tuples of 2 values consisting of \n",
    "    coeffs array_like Coefficients list of tuples:\n",
    "    [(cAn, cDn), ..., (cA2, cD2), (cA1, cD1)]\n",
    "    '''\n",
    "    reshape_list = []\n",
    "    for i in range(len(coeffs)):\n",
    "        reshape_list.append([])\n",
    "        for j in range(len(coeffs[0])):\n",
    "            reshape_list[i].append(tuple(coeffs[i][j]))\n",
    "            \n",
    "    return reshape_list\n",
    "\n",
    "def create_dataset(dataset, look_back):\n",
    "    dataX,dataY=[],[]\n",
    "\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a=dataset[i:(i+look_back),0:4]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+look_back,0:4])\n",
    "    return np.array(dataX),np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e46b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "    ''' https://arxiv.org/abs/1907.05321'''\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                              shape=(int(self.seq_len),),\n",
    "                                              initializer='uniform',\n",
    "                                              trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                           shape=(int(self.seq_len),),\n",
    "                                           initializer='uniform',\n",
    "                                           trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                                shape=(int(self.seq_len),),\n",
    "                                                initializer='uniform',\n",
    "                                                trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                             shape=(int(self.seq_len),),\n",
    "                                             initializer='uniform',\n",
    "                                             trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        x = tf.math.reduce_mean(x[:, :, :4], axis=-1)\n",
    "        time_linear = self.weights_linear * x + self.bias_linear\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79145b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(Layer):\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k,\n",
    "                           input_shape=input_shape,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.key = Dense(self.d_k,\n",
    "                         input_shape=input_shape,\n",
    "                         kernel_initializer='glorot_uniform',\n",
    "                         bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.value = Dense(self.d_v,\n",
    "                           input_shape=input_shape,\n",
    "                           kernel_initializer='glorot_uniform',\n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x / np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out\n",
    "\n",
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))\n",
    "        self.linear = Dense(input_shape[0][-1],\n",
    "                            input_shape=input_shape,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1)\n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):  # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config\n",
    "\n",
    "class TransformerDecoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=input_shape[0][-1], kernel_size=1, activation='relu')\n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):  # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer\n",
    "\n",
    "    def get_config(self):  # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c8c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  time_embedding = Time2Vector(seq_len)\n",
    "  layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "  layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "  layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "  layer4 = TransformerDecoder(d_k, d_v, n_heads, ff_dim)\n",
    "  layer5 = TransformerDecoder(d_k, d_v, n_heads, ff_dim)\n",
    "  in_seq = Input(shape=(seq_len, inp_len))\n",
    "\n",
    "  x = time_embedding(in_seq)\n",
    "  x = Concatenate(axis=-1)([in_seq, x])\n",
    "  x = layer1((x, x, x))\n",
    "  x = layer2((x, x, x))\n",
    "  x = layer3((x, x, x))\n",
    "  x = layer4((x, x, x))\n",
    "  x = layer5((x, x, x))\n",
    "  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "  x = Dropout(0.1)(x)\n",
    "  x = Dense(128, activation='relu')(x)\n",
    "  x = Dropout(0.1)(x)\n",
    "  out = Dense(out_len, activation='linear')(x)\n",
    "\n",
    "  model = Model(inputs=in_seq, outputs=out)\n",
    "  model.compile(loss='mse', optimizer='RMSProp', metrics=['mae', 'mape'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a55a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 1\n",
    "d_k = 256\n",
    "d_v = 256\n",
    "n_heads = 12\n",
    "ff_dim = 256\n",
    "lev=3\n",
    "inp_len=2*lev\n",
    "out_len=2*lev\n",
    "window=200\n",
    "look_back = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2009f68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum decomposition level: 5\n",
      "(36000,)\n",
      "<class 'numpy.ndarray'>\n",
      "(35800, 3, 2, 200)\n",
      "(35800, 1200)\n",
      "(35800, 6, 200)\n"
     ]
    }
   ],
   "source": [
    "# print(df.head())\n",
    "# print(df.dtypes)\n",
    "dataset = df['Volt-Ampere'].values\n",
    "dataset=dataset.astype('float32')\n",
    "\n",
    "# print(\"Dataset Shape:\", dataset.shape)\n",
    "# print(\"Dataset Length:\", len(dataset))\n",
    "\n",
    "# s = dataset[:12000*3]\n",
    "s = dataset[:12000*3]\n",
    "# s = np.squeeze(dataset[:12000*3], axis=1)  #\n",
    "\n",
    "# Get the maximum decomposition level\n",
    "max_level = pywt.swt_max_level(len(s))\n",
    "print(\"Maximum decomposition level:\", max_level)\n",
    "\n",
    "print(s.shape)\n",
    "\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "print(type(s))\n",
    "da=data_preparation(s, window, lev)\n",
    "# print(da[0][0])\n",
    "\n",
    "Vv = np.array(da)\n",
    "print(Vv.shape)\n",
    "# print(Vv[0][0])\n",
    "\n",
    "vv = Vv.reshape(Vv.shape[0],2*lev*Vv.shape[3])\n",
    "print(vv.shape)\n",
    "\n",
    "\n",
    "dataset = scaler.fit_transform(vv)\n",
    "\n",
    "dat = dataset.reshape(Vv.shape[0],2*lev,Vv.shape[3])\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e906be69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35800, 6, 200)\n",
      "(23867, 6, 1)\n",
      "(11933, 6, 1)\n",
      "(35800, 6, 1)\n",
      "(35800, 1, 6)\n",
      "(23867, 1, 6)\n",
      "(11933, 1, 6)\n"
     ]
    }
   ],
   "source": [
    "print(dat.shape)\n",
    "alpha=0.6667\n",
    "trainX,trainY=dat[:int(dat.shape[0]*alpha),:,window-seq_len-1:window-1],dat[:int(dat.shape[0]*alpha),:,window-1]\n",
    "testX,testY=dat[int(dat.shape[0]*alpha):,:,window-seq_len-1:window-1],dat[int(dat.shape[0]*alpha):,:,window-1]\n",
    "testX_a, testY_a = dat[:,:,window-seq_len-1:window-1],dat[:,:,window-1]\n",
    "\n",
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(testX_a.shape)\n",
    "\n",
    "testX_a=np.transpose(testX_a, (0, 2, 1))\n",
    "trainX=np.transpose(trainX, (0, 2, 1))\n",
    "testX =np.transpose(testX, (0, 2, 1))\n",
    "\n",
    "print(testX_a.shape)\n",
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1, 6)]       0           []                               \n",
      "                                                                                                  \n",
      " time2_vector (Time2Vector)     (None, 1, 2)         4           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1, 8)         0           ['input_1[0][0]',                \n",
      "                                                                  'time2_vector[0][0]']           \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, 1, 8)        111920      ['concatenate[0][0]',            \n",
      " erEncoder)                                                       'concatenate[0][0]',            \n",
      "                                                                  'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " transformer_encoder_1 (Transfo  (None, 1, 8)        111920      ['transformer_encoder[0][0]',    \n",
      " rmerEncoder)                                                     'transformer_encoder[0][0]',    \n",
      "                                                                  'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Transfo  (None, 1, 8)        111920      ['transformer_encoder_1[0][0]',  \n",
      " rmerEncoder)                                                     'transformer_encoder_1[0][0]',  \n",
      "                                                                  'transformer_encoder_1[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 1, 8)        107632      ['transformer_encoder_2[0][0]',  \n",
      " erDecoder)                                                       'transformer_encoder_2[0][0]',  \n",
      "                                                                  'transformer_encoder_2[0][0]']  \n",
      "                                                                                                  \n",
      " transformer_decoder_1 (Transfo  (None, 1, 8)        107632      ['transformer_decoder[0][0]',    \n",
      " rmerDecoder)                                                     'transformer_decoder[0][0]',    \n",
      "                                                                  'transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 1)           0           ['transformer_decoder_1[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1)            0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          256         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6)            774         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 552,058\n",
      "Trainable params: 552,058\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "# Training data\n",
    "X_train, y_train = trainX,trainY\n",
    "###############################################################################\n",
    "# Validation data\n",
    "X_val, y_val = testX,testY\n",
    "###############################################################################\n",
    "# Test data\n",
    "X_test, y_test = testX_a,testY_a\n",
    "callback = tf.keras.callbacks.ModelCheckpoint('Transformer_5min.hdf5',\n",
    "                                                      monitor='val_loss',\n",
    "                                                      save_best_only=True,\n",
    "                                                      verbose=1)\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    history = model.fit(X_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "#                             epochs=50,\n",
    "                            epochs=1,\n",
    "                            validation_data=(X_val, y_val),\n",
    "                            callbacks=[callback])\n",
    "\n",
    "model = tf.keras.models.load_model('Transformer_5min.hdf5',\n",
    "                                           custom_objects={'Time2Vector': Time2Vector,\n",
    "                                                           'SingleAttention': SingleAttention,\n",
    "                                                           'MultiAttention': MultiAttention,\n",
    "                                                           'TransformerEncoder': TransformerEncoder,\n",
    "                                                           'TransformerDecoder': TransformerDecoder}\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d5031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the whole signal (both train and validation data)\n",
    "# The metrics are computed only using the validation part.\n",
    "# This is needed for the signal processing\n",
    "print(testX_a.shape)\n",
    "\n",
    "testPredict_a =  model.predict(testX_a)\n",
    "\n",
    "print(testPredict_a.shape)\n",
    "d=dat\n",
    "d[:,:,window-1]=testPredict_a\n",
    "print(d.shape)\n",
    "D = d.reshape(d.shape[0],d.shape[1]*d.shape[2])\n",
    "print(D.shape)\n",
    "\n",
    "R = scaler.inverse_transform(D)\n",
    "R = R.reshape(d.shape[0],lev,2,d.shape[2])\n",
    "print(R.shape)\n",
    "\n",
    "R = data_organization(R)\n",
    "\n",
    "re=data_reconstruction(R, window)\n",
    "Re = np.array(re)\n",
    "print(Re.shape)\n",
    "\n",
    "tY = s[window:].reshape(s[window:].shape[0],1)\n",
    "print(tY.shape)\n",
    "P1 = Re.reshape(Re.shape[0],1)\n",
    "\n",
    "testYa=tY[int(tY.shape[0]*alpha):,:] # take only the validation part\n",
    "testPredicta = P1[int(tY.shape[0]*alpha):, :] # take only the validation part\n",
    "\n",
    "predicted_values, y_test=testPredicta[1:], testYa[:-1]\n",
    "test_rmse = math.sqrt( mean_squared_error(y_test, predicted_values))\n",
    "\n",
    "test_mae=mean_absolute_error(y_test, predicted_values)\n",
    "mape=100*np.mean(np.divide(abs(y_test- predicted_values),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e44900",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(y_test)\n",
    "plt.plot(predicted_values)\n",
    "plt.xlabel('Time/min')\n",
    "plt.ylabel('Electricity load (kWh)')\n",
    "plt.legend(['True', 'Predict'], loc='upper left')\n",
    "plt.show()\n",
    "print('RMSE:  %.4f' % test_rmse)\n",
    "print('MAE:  %.4f' % test_mae)\n",
    "print('MAPE:  %.4f' % mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff955bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([i for i in range(1000)])\n",
    "window = 200\n",
    "lev = 3\n",
    "\n",
    "# Get the maximum decomposition level\n",
    "max_level = pywt.swt_max_level(len(dataset))\n",
    "print(\"Maximum decomposition level:\", max_level)\n",
    "\n",
    "coeffs = data_preparation(dataset, window, lev)\n",
    "# print(np.array(coeffs).shape)\n",
    "# print(coeffs[0][0])\n",
    "coeffs = np.array(coeffs)\n",
    "# print(coeffs.shape)\n",
    "# print(coeffs[0][0])\n",
    "\n",
    "coeffs = data_organization(coeffs)\n",
    "\n",
    "print(np.array(coeffs).shape)\n",
    "res = data_reconstruction(coeffs,window)\n",
    "print(res)\n",
    "np.array(res).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce4d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved for full reconstruction\n",
    "def data_reconstruction(dataset,window):\n",
    "    da = []\n",
    "    for i in tqdm(range(len(dataset)), total= len(dataset), desc=\"iswt\"):\n",
    "#         recon = pywt.iswt(dataset[i,:,:,:].tolist(), 'db2')\n",
    "        recon = pywt.iswt(dataset[i], 'db2')\n",
    "#         da.append(recon[window-1])\n",
    "        da.append(recon)\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd21ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swt_env",
   "language": "python",
   "name": "swt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
