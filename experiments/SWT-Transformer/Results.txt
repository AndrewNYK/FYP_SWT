80% train results - before iswt
RMSE:  0.0228
MAE:  0.0156
MAPE:  61.9755

# Main and window
Summary: 5 epochs generally is enough, window does affect but not large

67% train results seq_len=1 window=200 5 epochs
RMSE:  0.0016
MAE:  0.0009
MAPE:  4.0947

# Window affects level of decomposition of swt
67% train results seq_len=1 window=1000 5 epochs
RMSE:  0.0019
MAE:  0.0011
MAPE:  4.5729

67% train results seq_len=1 window=80 15 epochs
RMSE:  0.0017
MAE:  0.0010
MAPE:  4.2491


# seq_len similar to lookback
Summary: lookback seems to improve the model but higher seq_len does not change much, increases computation time

67% train results seq_len=3 window=200 5 epochs
RMSE:  0.0013
MAE:  0.0007
MAPE:  2.9075

67% train results seq_len=10 window=200 5 epochs
RMSE:  0.0014
MAE:  0.0009
MAPE:  3.5401

67% train results seq_len=20 window=200 5 epoch
RMSE:  0.0013
MAE:  0.0008
MAPE:  2.9780


# Pytorch
67% train results seq_len=1 window=200 5 epochs pytorch
RMSE:  0.0056
MAE:  0.0037
MAPE:  15.0401

# parameters numbers are similar, but there seems to be a problem replicating
# Shape of tensors are similar with some transpose in pytorch (could be the issue with some conv layer processing)