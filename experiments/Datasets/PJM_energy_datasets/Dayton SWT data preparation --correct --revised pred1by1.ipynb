{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c52fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"./Models\")\n",
    "import os\n",
    "os.system('')\n",
    "\n",
    "import subprocess\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pickle\n",
    "import pgzip\n",
    "import copy\n",
    "\n",
    "import datetime\n",
    "\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2d59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default settings for experiment\n",
    "arg_model = \"tsrnn\" #Options: 'trfbb', 'tsrnn', 'trfbf'\n",
    "arg_dset = \"aep\" #Datasets -- Spain: 'ree', AEP, DAYTON: 'dyt' London: 'lsm'\n",
    "\n",
    "attr_dset_smpl_rt = 24 if arg_dset == \"aep\" else (48 if arg_dset == \"lsm\" else 24) #Samples per day. Spain, AEP: 24, London: 48\n",
    "param_dset_lookback_weeks = 5\n",
    "param_dset_forecast = 48 if arg_dset == \"lsm\" else 24\n",
    "# param_dset_lookback_weeks = 9\n",
    "# param_dset_forecast = 168 if arg_dset == \"lsm\" else 84 # 3.5days = 168\n",
    "param_dset_train_stride = 48 #Choose a coprime value to the forecast so all reading frames are eventually considered\n",
    "param_dset_test_stride = 'same' #tsrnn paper uses 1 week\n",
    "param_dset_lookback = param_dset_lookback_weeks*7*attr_dset_smpl_rt - param_dset_forecast\n",
    "\n",
    "param_trf_weather = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab7d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Get the maximum decomposition level\n",
    "def print_maximal_decom_level(data):\n",
    "    max_level = pywt.swt_max_level(data)\n",
    "#     print(\"Maximum decomposition level:\", max_level)\n",
    "    \n",
    "    return max_level\n",
    "\n",
    "# SWT functions\n",
    "def data_preparation(dataset, window, lev):\n",
    "    '''\n",
    "    Converts time series to wave\n",
    "    '''\n",
    "    da = []\n",
    "    coeffs = []\n",
    "    \n",
    "#     for i in tqdm(range(len(dataset)), total=len(dataset), desc=\"swt\"):\n",
    "    for i in range(len(dataset)):\n",
    "        for j in range(len(dataset[0])-window):\n",
    "            coeffs_j = pywt.swt(dataset[i][j:window+j], wavelet='db2', level=lev)\n",
    "            coeffs.append(coeffs_j)\n",
    "            \n",
    "        da.append(coeffs);\n",
    "        coeffs = []\n",
    "    return da\n",
    "\n",
    "def data_reconstruction(dataset, window):\n",
    "    '''\n",
    "    Converts wave back to time series\n",
    "    Take window-1 (last) value since we predict this value based on window\n",
    "    '''\n",
    "    full_recon = []\n",
    "#     for i in tqdm(range(len(dataset)), total= len(dataset), desc=\"iswt\"):\n",
    "    for i in range(len(dataset)):\n",
    "        da = []\n",
    "        for j in range(len(dataset[0])):\n",
    "            recon = pywt.iswt(dataset[i][j], 'db2')\n",
    "            da.append(recon[window-1])\n",
    "        full_recon.append(da)\n",
    "\n",
    "    return full_recon\n",
    "\n",
    "\n",
    "# Called because iswt cannot accept tolist() dataset\n",
    "def data_organization(data):\n",
    "    '''\n",
    "    Reshape data back to (n,m,3,2,window_length), \n",
    "    n number of different household, m number of timelines\n",
    "    where there are 3 tuples of 2 values consisting of \n",
    "    coeffs array_like Coefficients list of tuples:\n",
    "    [(cAn, cDn), ..., (cA2, cD2), (cA1, cD1)]\n",
    "    '''\n",
    "    full_reshape_list = []\n",
    "    for i in range(len(data)):\n",
    "        reshape_list = []\n",
    "        for j in range(len(data[0])):\n",
    "            reshape_list.append([])\n",
    "            for k in range(len(data[0][0])):\n",
    "                reshape_list[j].append(tuple(data[i][j][k]))\n",
    "        \n",
    "        full_reshape_list.append(reshape_list)    \n",
    "    \n",
    "    return full_reshape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed9883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dayton_def import DAYTON\n",
    "\n",
    "#Compute remaining settings\n",
    "param_dset_lookback = param_dset_lookback_weeks*7*attr_dset_smpl_rt - param_dset_forecast\n",
    "if param_dset_train_stride == 'same': param_dset_train_stride = param_dset_forecast\n",
    "if param_dset_test_stride == 'same': param_dset_test_stride = param_dset_forecast\n",
    "attr_dset_smpl_rt = {'ree':24,'aep':24,'lsm':48, 'dyt':24}[arg_dset]\n",
    "\n",
    "param_trf_inp_dim = {'ree':7,'lsm': 14}[arg_dset] if param_trf_weather else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0d3dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([471, 840, 1])\n"
     ]
    }
   ],
   "source": [
    "full_set = DAYTON(path = \".\",\n",
    "                  seq_len = param_dset_lookback,\n",
    "                  pred_horz = param_dset_forecast,\n",
    "                  timestamp = False)\n",
    "dytmax = full_set.max()\n",
    "dytmin = full_set.min()\n",
    "del(full_set)\n",
    "\n",
    "train_set = DAYTON(path = \".\",\n",
    "                start_idx = 0, end_idx = 97036,\n",
    "                seq_len = param_dset_lookback,\n",
    "                pred_horz = param_dset_forecast,\n",
    "                stride=29,\n",
    "                timestamp = False)\n",
    "val_set = DAYTON(path = \".\",\n",
    "              start_idx = 97036, end_idx = 97036+12129,\n",
    "                seq_len = param_dset_lookback,\n",
    "                pred_horz = param_dset_forecast,\n",
    "                stride=param_dset_forecast,\n",
    "                timestamp = False)\n",
    "test_set = DAYTON(path = \".\",\n",
    "                start_idx = 97036+12129,\n",
    "                seq_len = param_dset_lookback,\n",
    "                pred_horz = param_dset_forecast,\n",
    "                stride=param_dset_forecast,\n",
    "                timestamp = False)\n",
    "\n",
    "\n",
    "train_set.series = (train_set.series - dytmin)/(dytmax - dytmin)\n",
    "val_set.series = (val_set.series - dytmin)/(dytmax - dytmin)\n",
    "test_set.series = (test_set.series - dytmin)/(dytmax - dytmin)\n",
    "\n",
    "print(test_set.series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e36f9",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9abf1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b50fe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.weights_linear = nn.Parameter(torch.rand(seq_len, requires_grad=True))\n",
    "        self.bias_linear = nn.Parameter(torch.rand(seq_len), requires_grad=True)\n",
    "        self.weights_periodic = nn.Parameter(torch.rand(seq_len), requires_grad=True)\n",
    "        self.bias_periodic = nn.Parameter(torch.rand(seq_len), requires_grad=True)\n",
    "        \n",
    "        # Initialize parameters with uniform distribution\n",
    "        nn.init.uniform_(self.weights_linear, a=0.0, b=1.0)\n",
    "        nn.init.uniform_(self.bias_linear, a=0.0, b=1.0)\n",
    "        nn.init.uniform_(self.weights_periodic, a=0.0, b=1.0)\n",
    "        nn.init.uniform_(self.bias_periodic, a=0.0, b=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x[:, :, :], dim=-1)\n",
    "        time_linear = self.weights_linear * x + self.bias_linear\n",
    "        time_linear = time_linear.unsqueeze(-1)\n",
    "\n",
    "        time_periodic = torch.sin(x * self.weights_periodic + self.bias_periodic)\n",
    "        time_periodic = time_periodic.unsqueeze(-1)\n",
    "\n",
    "        return torch.cat([time_linear, time_periodic], dim=-1)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'seq_len={self.seq_len}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973b717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_v, inp_len):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.query = nn.Linear(in_features=inp_len, out_features=d_k)\n",
    "        nn.init.xavier_uniform_(self.query.weight)\n",
    "        nn.init.zeros_(self.query.bias)\n",
    "\n",
    "        self.key = nn.Linear(in_features=inp_len, out_features=d_k)\n",
    "        nn.init.xavier_uniform_(self.key.weight)\n",
    "        nn.init.zeros_(self.key.bias)\n",
    "\n",
    "        self.value = nn.Linear(in_features=inp_len, out_features=d_v)\n",
    "        nn.init.xavier_uniform_(self.value.weight)\n",
    "        nn.init.zeros_(self.value.bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn_weights = attn_weights / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = torch.matmul(attn_weights, v)\n",
    "        return attn_out\n",
    "    \n",
    "class MultiAttention(nn.Module):\n",
    "    def __init__(self, d_k, d_v, n_heads, inp_len):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.attn_heads = nn.ModuleList([SingleAttention(d_k, d_v, inp_len) for _ in range(n_heads)])\n",
    "        \n",
    "        self.linear = nn.Linear(d_k * n_heads, inp_len)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = torch.cat(attn, dim=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c516d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, seq_len, inp_len, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        self.attn_multi = MultiAttention(d_k, d_v, n_heads, inp_len)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.attn_normalize = nn.LayerNorm(normalized_shape=inp_len, eps=1e-6)\n",
    "\n",
    "#         self.ff_conv1D_1 = nn.Conv1d(in_channels=8, out_channels=self.ff_dim, kernel_size=1)\n",
    "        self.ff_conv1D_1 = nn.Conv1d(in_channels=seq_len, out_channels=self.ff_dim, kernel_size=1)\n",
    "#         self.ff_conv1D_2 = nn.Conv1d(in_channels=self.ff_dim, out_channels=8, kernel_size=1)\n",
    "        self.ff_conv1D_2 = nn.Conv1d(in_channels=self.ff_dim, out_channels=seq_len, kernel_size=1)\n",
    "        self.ff_dropout = nn.Dropout(dropout)\n",
    "        self.ff_normalize = nn.LayerNorm(normalized_shape=inp_len, eps=1e-6)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         print(len(inputs))\n",
    "#         print(inputs[0].shape)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        # Correction for transpose\n",
    "#         ff_layer = self.ff_conv1D_1(attn_layer.transpose(1, 2))\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = F.relu(ff_layer)\n",
    "#         ff_layer = self.ff_conv1D_2(ff_layer).transpose(1, 2)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, seq_len, inp_len, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        self.attn_multi = MultiAttention(d_k, d_v, n_heads, inp_len)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.attn_normalize = nn.LayerNorm(normalized_shape=inp_len, eps=1e-6)\n",
    "\n",
    "#         self.ff_conv1D_1 = nn.Conv1d(in_channels=8, out_channels=8, kernel_size=1)\n",
    "        self.ff_conv1D_1 = nn.Conv1d(in_channels=seq_len, out_channels=seq_len, kernel_size=1)\n",
    "        self.ff_dropout = nn.Dropout(dropout)\n",
    "        self.ff_normalize = nn.LayerNorm(normalized_shape=inp_len, eps=1e-6)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "        \n",
    "        # Transpose for pytorch implementation\n",
    "#         ff_layer = self.ff_conv1D_1(attn_layer.transpose(1, 2)).transpose(1, 2)\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = F.relu(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080fde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWT_Transformer(nn.Module):\n",
    "    def __init__(self, seq_len, inp_len, out_len, d_k, d_v, n_heads, ff_dim):\n",
    "        super(SWT_Transformer, self).__init__()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.time_embedding = Time2Vector(seq_len)\n",
    "        \n",
    "        self.layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim, seq_len, inp_len)\n",
    "        self.layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim, seq_len, inp_len)\n",
    "        self.layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim, seq_len, inp_len)\n",
    "        self.layer4 = TransformerDecoder(d_k, d_v, n_heads, ff_dim, seq_len, inp_len)\n",
    "        self.layer5 = TransformerDecoder(d_k, d_v, n_heads, ff_dim, seq_len, inp_len)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(seq_len, 32)\n",
    "        self.fc2 = nn.Linear(32, out_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_seq = x\n",
    "        \n",
    "        time_embedding = self.time_embedding(in_seq)\n",
    "        x = torch.cat([in_seq, time_embedding], dim=-1)\n",
    "        \n",
    "        x = self.layer1((x, x, x))\n",
    "        x = self.layer2((x, x, x))\n",
    "        x = self.layer3((x, x, x))\n",
    "        x = self.layer4((x, x, x))\n",
    "        x = self.layer5((x, x, x))\n",
    "\n",
    "        x = self.pooling(x).squeeze(2)\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f52ef6",
   "metadata": {},
   "source": [
    "## Create model and data tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c11a8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SWT Transformation\n",
    "def swt_transformation(self_series, window, lev):\n",
    "    self_series = self_series.squeeze(-1)\n",
    "#     print(\"total series shape after squeeze: \", self_series.shape)\n",
    "    self_series_numpy = self_series.numpy()\n",
    "#     print(\"torch shape: \", self_series.shape)\n",
    "#     print(\"numpy shape: \", self_series_numpy.shape)\n",
    "#     print(self_series[0])\n",
    "#     print(self_series_numpy[0])\n",
    "\n",
    "    da = data_preparation(self_series_numpy, window, lev)\n",
    "#     print(da[0][0])\n",
    "\n",
    "    Vv = np.array(da)\n",
    "#     print(Vv.shape)\n",
    "#     print(Vv[0][0])\n",
    "\n",
    "    vv = Vv.reshape(Vv.shape[0],Vv.shape[1],2*lev*Vv.shape[4])\n",
    "#     print(vv.shape)\n",
    "\n",
    "\n",
    "    # dataset = scaler.fit_transform(vv)\n",
    "\n",
    "    dat = vv.reshape(Vv.shape[0],Vv.shape[1],2*lev,Vv.shape[4])\n",
    "#     print(dat.shape)\n",
    "    \n",
    "    return dat\n",
    "    \n",
    "# Early stopping\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save or load the best model while training.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "\n",
    "    def __call__(\n",
    "        self, current_valid_loss, epoch, model\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "#             print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "#             print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            # Save the PyTorch model\n",
    "            torch.save(model.state_dict(), 'transformer_5min.pth')\n",
    "    \n",
    "patience = 15\n",
    "early_stopper = EarlyStopper(patience=patience, min_delta=0)\n",
    "saveBestModel = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d846fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = param_dset_lookback\n",
    "max_level = print_maximal_decom_level(window)\n",
    "lev = max_level if max_level < 3 else 3\n",
    "\n",
    "batch_size = 256\n",
    "d_k = 256\n",
    "d_v = 256\n",
    "n_heads = 12\n",
    "ff_dim = 256\n",
    "inp_len = 2*lev + 2 # 2 from time2Vector embedding\n",
    "out_len = 2*lev\n",
    "\n",
    "seq_len = 15\n",
    "\n",
    "# lev = 3\n",
    "# seq_len = 1\n",
    "# window = 200\n",
    "# look_back = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36934952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3318, 840, 1])\n",
      "torch.Size([471, 840, 1])\n",
      "torch.Size([471, 840, 1])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "train_set_copy = copy.deepcopy(train_set.series)\n",
    "val_set_copy = copy.deepcopy(val_set.series)\n",
    "test_set_copy = copy.deepcopy(test_set.series)\n",
    "\n",
    "print(train_set_copy.shape)\n",
    "print(val_set_copy.shape)\n",
    "print(val_set_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e410735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, targets are formed from the same mistakes of previous days\n",
    "# targets should be form from the original train_set.series for better training\n",
    "# Inputs however should come from changes of different days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb936bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual number of model parameters: 562903\n",
      "Trainable model parameters: 562903\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27354373f10a48b8a30039424ba472d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf821d3594da4be2bff6e6849d8da421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "forecast:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create PyTorch model\n",
    "model = SWT_Transformer(seq_len, inp_len, out_len, d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "# Print model summary\n",
    "# print(model)\n",
    "def num_parameters(m):\n",
    "    return sum([p.numel() for p in m.parameters()])\n",
    "\n",
    "parameters = num_parameters(model)\n",
    "\n",
    "# print(f\"Expected number of parameters: {m * dk * dk + m * 1 * 1 * n}\")\n",
    "print(f\"Actual number of model parameters: {parameters}\")\n",
    "\n",
    "trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print(f\"Trainable model parameters: {trainable_params}\" )\n",
    "\n",
    "# total_params = 0\n",
    "# for name, parameter in model.named_parameters():\n",
    "#     if not parameter.requires_grad:\n",
    "#         continue\n",
    "#     params = parameter.numel()\n",
    "#     print(f\"{name}, {params}\")\n",
    "#     total_params+=params\n",
    "# print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, eps=1e-07)\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "Grad_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Replace with your desired number of epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), total= num_epochs, desc=\"epochs\", position=0, leave=True):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    \n",
    "    for forecast_day in tqdm(range(param_dset_forecast), total=param_dset_forecast, desc=\"forecast\", position=1, leave=True):\n",
    "\n",
    "        train_set_transform = swt_transformation(train_set_copy[:,forecast_day:window+forecast_day+1], window, lev)\n",
    "#         print(\"train_set_transform shape: \", train_set_transform.shape)\n",
    "#         print(\"----------------------------------------------------\")\n",
    "\n",
    "        train_actual_transform = swt_transformation(train_set.series[:,forecast_day:window+forecast_day+1], window, lev)\n",
    "\n",
    "        val_set_transform = swt_transformation(val_set_copy[:,forecast_day:window+forecast_day+1], window, lev)\n",
    "#         print(\"val_set_transform shape: \", val_set_transform.shape)\n",
    "#         print(\"----------------------------------------------------\")\n",
    "\n",
    "        val_actual_transform = swt_transformation(val_set.series[:,forecast_day:window+forecast_day+1], window, lev)\n",
    "        \n",
    "        # Split seq_len and pred\n",
    "        train_set_transform = train_set_transform.squeeze(1)\n",
    "        val_set_transform = val_set_transform.squeeze(1)\n",
    "        \n",
    "        # Actual train and validation targets\n",
    "        train_actual_transform = train_actual_transform.squeeze(1)\n",
    "        val_actual_transform = val_actual_transform.squeeze(1)\n",
    "\n",
    "        # targets replaced with actual transformed data\n",
    "        trainX, trainY = train_set_transform[:,:,window-seq_len-1:window-1], train_actual_transform[:,:,window-1]\n",
    "        valX, valY = val_set_transform[:,:,window-seq_len-1:window-1], val_actual_transform[:,:,window-1]\n",
    "\n",
    "        trainX=np.transpose(trainX, (0, 2, 1))\n",
    "        valX =np.transpose(valX, (0, 2, 1))\n",
    "        \n",
    "        # Convert data to PyTorch tensors\n",
    "        X_train, y_train = torch.tensor(trainX), torch.tensor(trainY)\n",
    "        X_val, y_val = torch.tensor(valX), torch.tensor(valY)\n",
    "\n",
    "        # Create DataLoader for training and validation data\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        train_output = []\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device).nan_to_num(), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "    #             outputs = model(inputs)       \n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "                outputs = model(inputs)\n",
    "                loss = nn.MSELoss(reduction='mean')(outputs[~targets.isnan()], targets[~targets.isnan()])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                Grad_scaler.scale(loss).backward()\n",
    "                Grad_scaler.step(optimizer)\n",
    "                Grad_scaler.update()\n",
    "            \n",
    "            train_output.append(outputs.detach().cpu().numpy())  # Collect the outputs\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_output = []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device).nan_to_num(), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_output.append(outputs.cpu().numpy())  # Collect the outputs\n",
    "                loss = nn.MSELoss(reduction='mean')(outputs[~targets.isnan()], targets[~targets.isnan()])\n",
    "    #             val_loss += criterion(outputs, targets).item()\n",
    "                val_loss += loss\n",
    "                \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_concat = np.concatenate(train_output, axis=0)  # Concatenate outputs into a single numpy array\n",
    "        val_concat = np.concatenate(val_output, axis=0)  # Concatenate outputs into a single numpy array\n",
    "        \n",
    "        epoch_train_loss += train_loss\n",
    "        epoch_val_loss += val_loss\n",
    "        \n",
    "        # Function to iswt and modify train_set_copy\n",
    "        # Might have to calculate loss in each epoch rather than each forecast loop\n",
    "        # Result from test\n",
    "#         print(np.array(train_concat).shape)\n",
    "\n",
    "        train_transform_copy = train_set_transform.copy()\n",
    "#         print(train_transform_copy.shape)\n",
    "\n",
    "        # Prediction place at the end of the wave\n",
    "        # In iswt, we will take the last value as our prediction\n",
    "        train_set_transform[:,:,window-1] = train_concat\n",
    "#         print(train_set_transform.shape)\n",
    "\n",
    "        train_set_transform = train_set_transform.reshape(train_set_transform.shape[0],train_set_transform.shape[1]*train_set_transform.shape[2])\n",
    "#         print(train_set_transform.shape)\n",
    "        \n",
    "        train_set_transform = train_set_transform.reshape(train_transform_copy.shape[0],lev,2,train_transform_copy.shape[2])\n",
    "#         print(train_set_transform.shape)\n",
    "\n",
    "        train_set_transform = np.array(torch.from_numpy(train_set_transform).unsqueeze(1))\n",
    "#         print(train_set_transform.shape)\n",
    "\n",
    "        train_set_transform = data_organization(train_set_transform)\n",
    "\n",
    "        # Change to tuple type for iswt\n",
    "#         print(len(train_set_transform[0][0]))\n",
    "#         print(type(train_set_transform[0][0]))\n",
    "\n",
    "        train_result = data_reconstruction(train_set_transform, window)\n",
    "\n",
    "        # Prediction for next 48 data in test set\n",
    "        train_result = np.array(train_result)\n",
    "#         print(train_result.shape)\n",
    "        train_result = torch.from_numpy(train_result)\n",
    "        \n",
    "#         print(window+forecast_day)\n",
    "        train_set_copy[:,window+forecast_day] = train_result\n",
    "        \n",
    "        \n",
    "        # Function to iswt and modify train_set_copy\n",
    "        # Might have to calculate loss in each epoch rather than each forecast loop\n",
    "        # Result from test\n",
    "#         print(np.array(val_concat).shape)\n",
    "\n",
    "        val_transform_copy = val_set_transform.copy()\n",
    "#         print(val_transform_copy.shape)\n",
    "\n",
    "        # Prediction place at the end of the wave\n",
    "        # In iswt, we will take the last value as our prediction\n",
    "        val_set_transform[:,:,window-1] = val_concat\n",
    "#         print(val_set_transform.shape)\n",
    "\n",
    "        val_set_transform = val_set_transform.reshape(val_set_transform.shape[0],val_set_transform.shape[1]*val_set_transform.shape[2])\n",
    "#         print(val_set_transform.shape)\n",
    "        \n",
    "        val_set_transform = val_set_transform.reshape(val_transform_copy.shape[0],lev,2,val_transform_copy.shape[2])\n",
    "#         print(val_set_transform.shape)\n",
    "\n",
    "        val_set_transform = np.array(torch.from_numpy(val_set_transform).unsqueeze(1))\n",
    "#         print(val_set_transform.shape)\n",
    "\n",
    "        val_set_transform = data_organization(val_set_transform)\n",
    "\n",
    "        # Change to tuple type for iswt\n",
    "#         print(len(val_set_transform[0][0]))\n",
    "#         print(type(val_set_transform[0][0]))\n",
    "\n",
    "        val_result = data_reconstruction(val_set_transform, window)\n",
    "\n",
    "        # Prediction for next 48 data in test set\n",
    "        val_result = np.array(val_result)\n",
    "#         print(val_result.shape)\n",
    "        val_result = torch.from_numpy(val_result)\n",
    "        \n",
    "#         print(window+forecast_day)\n",
    "        val_set_copy[:,window+forecast_day] = val_result\n",
    "        \n",
    "    epoch_train_loss /= param_dset_forecast\n",
    "    epoch_val_loss /= param_dset_forecast\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_train_loss:.6f}, Validation Loss: {epoch_val_loss:.6f}')\n",
    "    \n",
    "    saveBestModel(epoch_val_loss, epoch, model)\n",
    "    \n",
    "    # Early stopping with patience\n",
    "    if early_stopper.early_stop(epoch_val_loss):\n",
    "        print(\"Done!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2bf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PyTorch model\n",
    "loaded_model = SWT_Transformer(seq_len, inp_len, out_len, d_k, d_v, n_heads, ff_dim)\n",
    "loaded_model.load_state_dict(torch.load('transformer_5min.pth'))\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "total_test_loss = 0.0\n",
    "\n",
    "for forecast_day in tqdm(range(param_dset_forecast), total=param_dset_forecast, desc=\"forecast\", position=1, leave=True):\n",
    "\n",
    "    test_set_transform = swt_transformation(test_set_copy[:,forecast_day:window+forecast_day+1], window, lev)\n",
    "#         print(\"test_set_transform shape: \", test_set_transform.shape)\n",
    "#         print(\"----------------------------------------------------\")\n",
    "    \n",
    "    test_actual_transform = swt_transformation(test_set.series[:,forecast_day:window+forecast_day+1], window, lev)\n",
    "\n",
    "    # Squeeze since predicting 1 day at a time\n",
    "    test_set_transform = test_set_transform.squeeze(1)\n",
    "    test_actual_transform = test_actual_transform.squeeze(1)\n",
    "\n",
    "    testX, testY = test_set_transform[:,:,window-seq_len-1:window-1], test_actual_transform[:,:,window-1]\n",
    "\n",
    "    testX = np.transpose(testX, (0, 2, 1))\n",
    "    \n",
    "    X_test, y_test = torch.tensor(testX), torch.tensor(testY)\n",
    "\n",
    "    \n",
    "    # Create DataLoader for testing data\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Testing the model on the test dataset\n",
    "    test_outputs = []\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device).nan_to_num(), targets.to(device)\n",
    "            outputs = loaded_model(inputs)\n",
    "            test_outputs.append(outputs.cpu().numpy())  # Collect the outputs\n",
    "\n",
    "            loss = nn.MSELoss(reduction='mean')(outputs[~targets.isnan()], targets[~targets.isnan()])\n",
    "            test_loss += loss\n",
    "    #         test_loss += criterion(outputs, targets).item()\n",
    "\n",
    "    test_concat = np.concatenate(test_outputs, axis=0)  # Concatenate outputs into a single numpy array\n",
    "    test_loss /= len(test_loader)\n",
    "    total_test_loss += test_loss\n",
    "    print(f'Test Loss: {test_loss:.6f}')\n",
    "    \n",
    "    # Function to iswt and modify train_set_copy\n",
    "    # Might have to calculate loss in each epoch rather than each forecast loop\n",
    "    # Result from test\n",
    "#     print(np.array(test_concat).shape)\n",
    "\n",
    "    test_transform_copy = test_set_transform.copy()\n",
    "#     print(test_transform_copy.shape)\n",
    "\n",
    "    # Prediction place at the end of the wave\n",
    "    # In iswt, we will take the last value as our prediction\n",
    "    test_set_transform[:,:,window-1] = test_concat\n",
    "#     print(test_set_transform.shape)\n",
    "\n",
    "    test_set_transform = test_set_transform.reshape(test_set_transform.shape[0],test_set_transform.shape[1]*test_set_transform.shape[2])\n",
    "#     print(test_set_transform.shape)\n",
    "\n",
    "    test_set_transform = test_set_transform.reshape(test_transform_copy.shape[0],lev,2,test_transform_copy.shape[2])\n",
    "#     print(test_set_transform.shape)\n",
    "\n",
    "    test_set_transform = np.array(torch.from_numpy(test_set_transform).unsqueeze(1))\n",
    "#     print(test_set_transform.shape)\n",
    "\n",
    "    test_set_transform = data_organization(test_set_transform)\n",
    "\n",
    "    # Change to tuple type for iswt\n",
    "#     print(len(test_set_transform[0][0]))\n",
    "#     print(type(test_set_transform[0][0]))\n",
    "\n",
    "    test_result = data_reconstruction(test_set_transform, window)\n",
    "\n",
    "    # Prediction for next 48 data in test set\n",
    "    test_result = np.array(test_result)\n",
    "#     print(test_result.shape)\n",
    "    test_result = torch.from_numpy(test_result)\n",
    "\n",
    "#     print(window+forecast_day)\n",
    "#     print(test_set_copy[:,window+forecast_day][0])\n",
    "#     print(test_result[0])\n",
    "    test_set_copy[:,window+forecast_day] = test_result\n",
    "#     print(test_set_copy[:,window+forecast_day][0])\n",
    "#     print(test_set_copy.shape)\n",
    "\n",
    "average_test_loss = total_test_loss / param_dset_forecast\n",
    "print(f'Average test Loss: {average_test_loss:.6f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004014ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Data\n",
    "test_actual = test_set.series[:, param_dset_lookback:]\n",
    "print(test_actual.shape)\n",
    "\n",
    "test_actual = test_actual.squeeze(-1)\n",
    "print(test_actual.shape)\n",
    "\n",
    "test_predict = test_set_copy[:, param_dset_lookback:]\n",
    "print(test_predict.shape)\n",
    "\n",
    "test_predict = test_predict.squeeze(-1)\n",
    "print(test_predict.shape)\n",
    "\n",
    "# This was included in the original transformer_houses1to5_5min program\n",
    "# Re, test_actual=Re[:,1:], test_actual[:,:-1] # I don't understand\n",
    "# Similarly, taking the values 1 time step before results in the equally good RMSE value\n",
    "# test_actual = test_set[:, param_dset_lookback-1:param_dset_lookback-1+param_dset_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input contains Nan\n",
    "# test_rmse = math.sqrt( mean_squared_error(test_actual, Re))\n",
    "# test_mae=mean_absolute_error(test_actual, Re)\n",
    "# mape=100*np.mean(np.divide(abs(test_actual- Re),test_actual))\n",
    "\n",
    "test_rmse = torch.nn.MSELoss(reduction='none')(test_actual, test_predict).nanmean().sqrt_()\n",
    "\n",
    "test_mae = (test_predict-test_actual).abs_().nanmean()\n",
    "\n",
    "mape = (2*(test_actual-test_predict).abs_() / (test_actual.abs() + test_predict.abs())).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6df351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE:  %.6f' % test_rmse)\n",
    "print('MAE:  %.6f' % test_mae)\n",
    "print('MAPE:  %.6f' % mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1d17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_actual[0])\n",
    "plt.plot(test_predict[0])\n",
    "plt.xlabel('Time/hr')\n",
    "plt.ylabel('AEP prediction units(tentative)')\n",
    "plt.legend(['True', 'Predict'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eab824",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_actual[1])\n",
    "plt.plot(test_predict[1])\n",
    "plt.xlabel('Time/hr')\n",
    "plt.ylabel('AEP prediction units(tentative)')\n",
    "plt.legend(['True', 'Predict'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(test_actual[200])\n",
    "plt.plot(test_predict[200])\n",
    "plt.xlabel('Time/hr')\n",
    "plt.ylabel('AEP prediction units(tentative)')\n",
    "plt.legend(['True', 'Predict'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218aaab",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swt_env",
   "language": "python",
   "name": "swt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
