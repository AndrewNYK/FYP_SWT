{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffacd22-abe1-44b3-a11d-5a3c7e263931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f31c05a5-3259-4477-baf9-13cfeb0f36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.metrics import mape, mase, rmse, mae, smape, mse\n",
    "\n",
    "from darts.utils.missing_values import fill_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f35ca-8e2a-420c-8d38-b996bafd0924",
   "metadata": {},
   "source": [
    "## MODWT functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b14ae0-c1f4-45e8-ba2f-5806264a7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import pywt\n",
    "\n",
    "\n",
    "def upArrow_op(li, j):\n",
    "    if j == 0:\n",
    "        return [1]\n",
    "    N = len(li)\n",
    "    li_n = np.zeros(2 ** (j - 1) * (N - 1) + 1)\n",
    "    for i in range(N):\n",
    "        li_n[2 ** (j - 1) * i] = li[i]\n",
    "    return li_n\n",
    "\n",
    "\n",
    "def period_list(li, N):\n",
    "    n = len(li)\n",
    "    # append [0 0 ...]\n",
    "    n_app = N - np.mod(n, N)\n",
    "    li = list(li)\n",
    "    li = li + [0] * n_app\n",
    "    if len(li) < 2 * N:\n",
    "        return np.array(li)\n",
    "    else:\n",
    "        li = np.array(li)\n",
    "        li = np.reshape(li, [-1, N])\n",
    "        li = np.sum(li, axis=0)\n",
    "        return li\n",
    "\n",
    "\n",
    "def circular_convolve_mra( signal, ker ):\n",
    "    '''\n",
    "        signal: real 1D array\n",
    "        ker: real 1D array\n",
    "        signal and ker must have same shape\n",
    "        Modification of \n",
    "            https://stackoverflow.com/questions/35474078/python-1d-array-circular-convolution\n",
    "    '''\n",
    "    return np.flip(np.real(np.fft.ifft( np.fft.fft(signal)*np.fft.fft(np.flip(ker))))).astype(np.int).tolist()\n",
    "\n",
    "\n",
    "def circular_convolve_d(h_t, v_j_1, j):\n",
    "    '''\n",
    "    jth level decomposition\n",
    "    h_t: \\tilde{h} = h / sqrt(2)\n",
    "    v_j_1: v_{j-1}, the (j-1)th scale coefficients\n",
    "    return: w_j (or v_j)\n",
    "    '''\n",
    "    N = len(v_j_1)\n",
    "    L = len(h_t)\n",
    "    w_j = np.zeros(N)\n",
    "    l = np.arange(L)\n",
    "    for t in range(N):\n",
    "        index = np.mod(t - 2 ** (j - 1) * l, N)\n",
    "        v_p = np.array([v_j_1[ind] for ind in index])\n",
    "        w_j[t] = (np.array(h_t) * v_p).sum()\n",
    "    return w_j\n",
    "\n",
    "\n",
    "def circular_convolve_s(h_t, g_t, w_j, v_j, j):\n",
    "    '''\n",
    "    (j-1)th level synthesis from w_j, w_j\n",
    "    see function circular_convolve_d\n",
    "    '''\n",
    "    N = len(v_j)\n",
    "    L = len(h_t)\n",
    "    v_j_1 = np.zeros(N)\n",
    "    l = np.arange(L)\n",
    "    for t in range(N):\n",
    "        index = np.mod(t + 2 ** (j - 1) * l, N)\n",
    "        w_p = np.array([w_j[ind] for ind in index])\n",
    "        v_p = np.array([v_j[ind] for ind in index])\n",
    "        v_j_1[t] = (np.array(h_t) * w_p).sum()\n",
    "        v_j_1[t] = v_j_1[t] + (np.array(g_t) * v_p).sum()\n",
    "    return v_j_1\n",
    "\n",
    "\n",
    "def modwt(x, filters, level):\n",
    "    '''\n",
    "    filters: 'db1', 'db2', 'haar', ...\n",
    "    return: see matlab\n",
    "    '''\n",
    "    # filter\n",
    "    wavelet = pywt.Wavelet(filters)\n",
    "    h = wavelet.dec_hi\n",
    "    g = wavelet.dec_lo\n",
    "    h_t = np.array(h) / np.sqrt(2)\n",
    "    g_t = np.array(g) / np.sqrt(2)\n",
    "    wavecoeff = []\n",
    "    v_j_1 = x\n",
    "    for j in range(level):\n",
    "        w = circular_convolve_d(h_t, v_j_1, j + 1)\n",
    "        v_j_1 = circular_convolve_d(g_t, v_j_1, j + 1)\n",
    "        wavecoeff.append(w)\n",
    "    wavecoeff.append(v_j_1)\n",
    "    return np.vstack(wavecoeff)\n",
    "\n",
    "\n",
    "def imodwt(w, filters):\n",
    "    ''' inverse modwt '''\n",
    "    # filter\n",
    "    wavelet = pywt.Wavelet(filters)\n",
    "    h = wavelet.dec_hi\n",
    "    g = wavelet.dec_lo\n",
    "    h_t = np.array(h) / np.sqrt(2)\n",
    "    g_t = np.array(g) / np.sqrt(2)\n",
    "    level = len(w) - 1\n",
    "    v_j = w[-1]\n",
    "    for jp in range(level):\n",
    "        j = level - jp - 1\n",
    "        v_j = circular_convolve_s(h_t, g_t, w[j], v_j, j + 1)\n",
    "    return v_j\n",
    "\n",
    "\n",
    "def modwtmra(w, filters):\n",
    "    ''' Multiresolution analysis based on MODWT'''\n",
    "    # filter\n",
    "    wavelet = pywt.Wavelet(filters)\n",
    "    h = wavelet.dec_hi\n",
    "    g = wavelet.dec_lo\n",
    "    # D\n",
    "    level, N = w.shape\n",
    "    level = level - 1\n",
    "    D = []\n",
    "    g_j_part = [1]\n",
    "    for j in range(level):\n",
    "        # g_j_part\n",
    "        g_j_up = upArrow_op(g, j)\n",
    "        g_j_part = np.convolve(g_j_part, g_j_up)\n",
    "        # h_j_o\n",
    "        h_j_up = upArrow_op(h, j + 1)\n",
    "        h_j = np.convolve(g_j_part, h_j_up)\n",
    "        h_j_t = h_j / (2 ** ((j + 1) / 2.))\n",
    "        if j == 0: h_j_t = h / np.sqrt(2)\n",
    "        h_j_t_o = period_list(h_j_t, N)\n",
    "        D.append(circular_convolve_mra(h_j_t_o, w[j]))\n",
    "    # S\n",
    "    j = level - 1\n",
    "    g_j_up = upArrow_op(g, j + 1)\n",
    "    g_j = np.convolve(g_j_part, g_j_up)\n",
    "    g_j_t = g_j / (2 ** ((j + 1) / 2.))\n",
    "    g_j_t_o = period_list(g_j_t, N)\n",
    "    S = circular_convolve_mra(g_j_t_o, w[-1])\n",
    "    D.append(S)\n",
    "    return np.vstack(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7116f9ee-964d-48ac-9808-b71b010ac041",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcdf62c4-7d1f-4597-81a6-14539c7b8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default settings for experiment\n",
    "arg_model = \"tsrnn\" #Options: 'trfbb', 'tsrnn', 'trfbf'\n",
    "arg_dset = \"dyt\" #Datasets -- Spain: 'ree', AEP, DAYTON: 'dyt' London: 'lsm'\n",
    "\n",
    "attr_dset_smpl_rt = 24 if arg_dset == \"aep\" else (48 if arg_dset == \"lsm\" else 24) #Samples per day. Spain, AEP: 24, London: 48\n",
    "param_dset_lookback_weeks = 5\n",
    "param_dset_forecast = 48 if arg_dset == \"lsm\" else 24\n",
    "# param_dset_lookback_weeks = 9\n",
    "# param_dset_forecast = 168 if arg_dset == \"lsm\" else 84 # 3.5days = 168\n",
    "param_dset_train_stride = 48 #Choose a coprime value to the forecast so all reading frames are eventually considered\n",
    "param_dset_test_stride = 'same' #tsrnn paper uses 1 week\n",
    "param_dset_lookback = param_dset_lookback_weeks*7*attr_dset_smpl_rt - param_dset_forecast\n",
    "\n",
    "param_trf_weather = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5aa8d7-6341-4883-84fb-6f7c7dfae455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PJM_energy_datasets.dayton_def import DAYTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3652aac7-9baa-4229-aad2-e30bb734d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3318, 840, 1])\n"
     ]
    }
   ],
   "source": [
    "full_set = DAYTON(path = \"PJM_energy_datasets/\",\n",
    "                  seq_len = param_dset_lookback,\n",
    "                  pred_horz = param_dset_forecast,\n",
    "                  timestamp = False)\n",
    "dytmax = full_set.max()\n",
    "dytmin = full_set.min()\n",
    "del(full_set)\n",
    "\n",
    "train_set = DAYTON(path = \"PJM_energy_datasets/\",\n",
    "                start_idx = 0, end_idx = 97036,\n",
    "                seq_len = param_dset_lookback,\n",
    "                pred_horz = param_dset_forecast,\n",
    "                stride=29,\n",
    "                timestamp = False)\n",
    "val_set = DAYTON(path = \"PJM_energy_datasets/\",\n",
    "              start_idx = 97036, end_idx = 97036+12129,\n",
    "                seq_len = param_dset_lookback,\n",
    "                pred_horz = param_dset_forecast,\n",
    "                stride=param_dset_forecast,\n",
    "                timestamp = False)\n",
    "test_set = DAYTON(path = \"PJM_energy_datasets/\",\n",
    "                start_idx = 97036+12129,\n",
    "                seq_len = param_dset_lookback,\n",
    "                pred_horz = param_dset_forecast,\n",
    "                stride=param_dset_forecast,\n",
    "                timestamp = False)\n",
    "\n",
    "\n",
    "train_set.series = (train_set.series - dytmin)/(dytmax - dytmin)\n",
    "val_set.series = (val_set.series - dytmin)/(dytmax - dytmin)\n",
    "test_set.series = (test_set.series - dytmin)/(dytmax - dytmin)\n",
    "\n",
    "print(train_set.series.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6190299-730e-4e2d-82ef-a58c66ea0464",
   "metadata": {},
   "source": [
    "## Import Models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c243c0dd-d64e-4791-8bf7-3c067c175448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "#Importing the testing models\n",
    "from darts.models import (\n",
    "    NaiveSeasonal,\n",
    "    NaiveDrift,\n",
    "    Prophet,\n",
    "    ExponentialSmoothing,\n",
    "    ARIMA,\n",
    "    AutoARIMA,\n",
    "    VARIMA,\n",
    "    BATS,\n",
    "    TBATS,\n",
    "    StatsForecastAutoARIMA,\n",
    "    RegressionEnsembleModel,\n",
    "    RegressionModel,\n",
    "    Theta,\n",
    "    FourTheta,\n",
    "    FFT,\n",
    "    NBEATSModel,\n",
    "    TFTModel,\n",
    "    RNNModel,\n",
    "    TransformerModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d2291b2-7f2b-4ea4-ba97-bf049a2e22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, train, val, f=None ):\n",
    "    model.fit(train)\n",
    "    forecast = model.predict(len(val))\n",
    "    mseRes = mse(val, forecast)\n",
    "    rmseRes = rmse(val, forecast)\n",
    "    # mapeRes = mape(val, forecast)\n",
    "    maeRes = mae(val, forecast)\n",
    "    smapeRes = smape(val, forecast)\n",
    "    # maseRes = mase(val, forecast,train)\n",
    "    \n",
    "    # print('model {} obtains MSE: {:.6f}'.format(model,mseRes))\n",
    "    # print('model {} obtains RMSE: {:.6f}'.format(model,rmseRes))\n",
    "    # # print('model {} obtains MAPE: {:.6f}'.format(model,mapeRes ))\n",
    "    # print('model {} obtains MAE: {:.6f}'.format(model, maeRes))\n",
    "    # print('model {} obtains SMAPE: {:.6f}'.format(model,smapeRes ))\n",
    "    # print('model {} obtains MASE: {:.6f}'.format(model,maseRes ))\n",
    "\n",
    "    return forecast, mseRes, rmseRes, maeRes, smapeRes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6179e24-9f90-4695-94f1-ae4cffca11a5",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384bcecb-500f-4cd1-ab75-9cffd802dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_squeeze = train_set.series.squeeze(-1)\n",
    "# print(train_set_squeeze.shape)\n",
    "train_np = train_set_squeeze.numpy()\n",
    "# print((type(train_np)))\n",
    "\n",
    "test_set_squeeze = test_set.series.squeeze(-1)\n",
    "# print(train_set_squeeze.shape)\n",
    "test_np = test_set_squeeze.numpy()\n",
    "\n",
    "test_np = test_np[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fcaa3d4-0e9d-4306-9d39-6a9a0ccb3c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962bc40a4b474a1c97876bd6cb1a83a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Series Conversion:   0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_series = np.nan_to_num(train_np[:100])\n",
    "train_series = np.nan_to_num(test_np)\n",
    "\n",
    "series_Lists = []\n",
    "train_set_series = []\n",
    "test_set_series = []\n",
    "val_set_series = []\n",
    "\n",
    "for i in tqdm(range(len(train_series)), total=len(train_series), desc=\"Series Conversion\", position=1, leave=True):\n",
    "    seriesList = []\n",
    "    train = []\n",
    "    test = []\n",
    "    val = []\n",
    "    nb_time = len(train_series[i])\n",
    "    # nb_val = int(nb_time*0.2)\n",
    "    \n",
    "    wt = modwt(train_series[i], 'db2', int(np.log(len(train_series[i]))))\n",
    "    for j in range(len(wt)):\n",
    "        \n",
    "        wt_df = TimeSeries.from_dataframe(pd.DataFrame(wt[j]))\n",
    "        seriesList.append(wt_df)\n",
    "        \n",
    "        wt_df_train = wt_df[:nb_time-param_dset_forecast]\n",
    "        train.append(wt_df_train)\n",
    "        \n",
    "        #wt_df_test = wt_df[nb_time-nforecast-nb_val:nb_time-nforecast]\n",
    "        #test.append(wt_df_test)\n",
    "        \n",
    "        wt_df_val = wt_df[nb_time-param_dset_forecast:]\n",
    "        val.append(wt_df_val)\n",
    "\n",
    "    series_Lists.append(seriesList)\n",
    "    train_set_series.append(train)\n",
    "    # test_set_series.append(test)\n",
    "    val_set_series.append(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f81bae-3cb8-47b2-a990-772e124988d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# series = train_np[0]\n",
    "# wt = modwt(series, 'haar', int(np.log(len(series))))\n",
    "# seriesList = []\n",
    "# train = []\n",
    "# test = []\n",
    "# val = []\n",
    "# nb_time = len(series)\n",
    "# # nb_val = int(nb_time*0.2)\n",
    "# # print(wt)\n",
    "\n",
    "# for i in range(len(wt)):\n",
    "#     # print(wt[i])\n",
    "#     wt_df = TimeSeries.from_dataframe(pd.DataFrame(wt[i]))\n",
    "#     seriesList.append(wt_df)\n",
    "    \n",
    "#     wt_df_train = wt_df[:nb_time-param_dset_forecast]\n",
    "#     train.append(wt_df_train)\n",
    "    \n",
    "#     #wt_df_test = wt_df[nb_time-nforecast-nb_val:nb_time-nforecast]\n",
    "#     #test.append(wt_df_test)\n",
    "    \n",
    "#     wt_df_val = wt_df[nb_time-param_dset_forecast:]\n",
    "#     val.append(wt_df_val)\n",
    "\n",
    "# print(len(train))\n",
    "# print(len(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4820efde-624d-492f-a8eb-e5c5bae4ea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471\n",
      "7\n",
      "816\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'darts.timeseries.TimeSeries'>\n",
      "816\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set_series))\n",
    "print(len(train_set_series[0]))\n",
    "print(len(train_set_series[0][0]))\n",
    "\n",
    "print(type(train_set_series))\n",
    "print(type(train_set_series[0]))\n",
    "print(type(train_set_series[0][0]))\n",
    "\n",
    "test = train_set_series[0][0]\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf6bb7b-67c1-4e11-92f8-05f0909b2b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "# Due to TimeSeries object, cannot use np.array\n",
    "# Current shape: 100,7,816 -> 7 * 100,816\n",
    "train_emb = []\n",
    "Series_List_emb = []\n",
    "\n",
    "for i in range(len(train_set_series[0])):\n",
    "    train_emb_temp = [t[i] for t in train_set_series]\n",
    "    train_emb.append(train_emb_temp)\n",
    "\n",
    "for i in range(len(series_Lists[0])):\n",
    "    series_temp = [s[i] for s in series_Lists]\n",
    "    Series_List_emb.append(series_temp)\n",
    "\n",
    "print(len(train_emb))\n",
    "print(len(Series_List_emb[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dce2227b-98f3-4e59-b9c0-65fa735404d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 6GB Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                | Params\n",
      "------------------------------------------------------------\n",
      "0 | criterion           | MSELoss             | 0     \n",
      "1 | train_metrics       | MetricCollection    | 0     \n",
      "2 | val_metrics         | MetricCollection    | 0     \n",
      "3 | encoder             | Linear              | 128   \n",
      "4 | positional_encoding | _PositionalEncoding | 0     \n",
      "5 | transformer         | Transformer         | 167 K \n",
      "6 | decoder             | Linear              | 65    \n",
      "------------------------------------------------------------\n",
      "167 K     Trainable params\n",
      "0         Non-trainable params\n",
      "167 K     Total params\n",
      "0.671     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be6e0bd8ce847cdbd7a7d72b3ef64e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_emb)):\n\u001b[0;32m     16\u001b[0m     transformers \u001b[38;5;241m=\u001b[39m TransformerModel(\n\u001b[0;32m     17\u001b[0m     input_chunk_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m     18\u001b[0m     output_chunk_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     pl_trainer_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: [my_stopper]}\n\u001b[0;32m     39\u001b[0m     )\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_emb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# print('seriesList = ', Series_List_emb[i])\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb-time-nforecast = \u001b[39m\u001b[38;5;124m'\u001b[39m,nb_time\u001b[38;5;241m-\u001b[39mparam_dset_forecast)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\darts\\utils\\torch.py:112\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m    111\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decorated(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:751\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit\u001b[1;34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, num_loader_workers)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    747\u001b[0m     series\u001b[38;5;241m=\u001b[39mseq2series(series),\n\u001b[0;32m    748\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39mseq2series(past_covariates),\n\u001b[0;32m    749\u001b[0m     future_covariates\u001b[38;5;241m=\u001b[39mseq2series(future_covariates),\n\u001b[0;32m    750\u001b[0m )\n\u001b[1;32m--> 751\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_from_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\darts\\utils\\torch.py:112\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[0;32m    111\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decorated(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:956\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit_from_dataset\u001b[1;34m(self, train_dataset, val_dataset, trainer, verbose, epochs, num_loader_workers)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_from_dataset\u001b[39m(\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m     num_loader_workers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    913\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchForecastingModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    914\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    Train the model with a specific :class:`darts.utils.data.TrainingDataset` instance.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;124;03m        Fitted model.\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 956\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_for_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_loader_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_loader_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\darts\\models\\forecasting\\torch_forecasting_model.py:1101\u001b[0m, in \u001b[0;36mTorchForecastingModel._train\u001b[1;34m(self, trainer, model, train_loader, val_loader)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_training:\n\u001b[1;32m-> 1101\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m trainer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:206\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:378\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    376\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitoring_callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    377\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_epoch_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39m_num_ready_batches_reached():\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:208\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[1;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m             fn(trainer, trainer\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py:190\u001b[0m, in \u001b[0;36mEarlyStopping.on_train_epoch_end\u001b[1;34m(self, trainer, pl_module)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_on_train_epoch_end \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_skip_check(trainer):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_early_stopping_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py:202\u001b[0m, in \u001b[0;36mEarlyStopping._run_early_stopping_check\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m logs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mfast_dev_run \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_condition_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# disable early_stopping with fast_dev_run\u001b[39;49;00m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# short circuit if metric not present\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    207\u001b[0m current \u001b[38;5;241m=\u001b[39m logs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\swt_env\\lib\\site-packages\\pytorch_lightning\\callbacks\\early_stopping.py:153\u001b[0m, in \u001b[0;36mEarlyStopping._validate_condition_metric\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m monitor_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict:\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    155\u001b[0m         rank_zero_warn(error_msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\n",
    "# a period of 2 epochs (`patience`)\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"train_loss\",\n",
    "    patience=1,\n",
    "    min_delta=0.0,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Train models based on different values of wavelet transform\n",
    "prediction = []\n",
    "models_transformers = []\n",
    "for i in range(len(train_emb)):\n",
    "    transformers = TransformerModel(\n",
    "    input_chunk_length=12,\n",
    "    output_chunk_length=1,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    model_name=\"transformer\"+str(i),\n",
    "    nr_epochs_val_period=1,\n",
    "    #d_model=16,\n",
    "    #nhead=8,\n",
    "    d_model=64,\n",
    "    nhead=32,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.1,\n",
    "    activation=\"relu\",\n",
    "    random_state=42,\n",
    "    save_checkpoints=True,\n",
    "    force_reset=True,\n",
    "    # pl_trainer_kwargs = {\"accelerator\": \"gpu\", \n",
    "    #                    \"gpus\": -1, \n",
    "    #                    \"auto_select_gpus\": True},\n",
    "    pl_trainer_kwargs = {\"callbacks\": [my_stopper]}\n",
    "    )\n",
    "    \n",
    "    transformers.fit(series = train_emb[i], verbose=True)\n",
    "    # print('seriesList = ', Series_List_emb[i])\n",
    "    print('nb-time-nforecast = ',nb_time-param_dset_forecast)\n",
    "    \n",
    "    pred_series = transformers.historical_forecasts(Series_List_emb[i],\n",
    "                                                    start = nb_time-param_dset_forecast, #the first point in time at which a prediction is computed\n",
    "                                                    retrain=False,\n",
    "                                                    verbose=True,\n",
    "                                                    )\n",
    "    prediction.append(pred_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f848a-fe69-4db0-b480-a1af18d500d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to TimeSeries object, cannot use np.array\n",
    "# Current shape: 7 * 100,816-> 100,7,816\n",
    "new_pred = []\n",
    "\n",
    "for i in range(len(prediction[0])):\n",
    "    pred_temp = [t[i] for t in prediction]\n",
    "    new_pred.append(pred_temp)\n",
    "\n",
    "print(len(new_pred))\n",
    "print(len(new_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bde66-06ec-4720-9e8e-ac8e43f2e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_result = []\n",
    "\n",
    "for series_num in range(len(new_pred)):\n",
    "    prediction_tmp = new_pred[series_num][0].pd_dataframe()\n",
    "    \n",
    "    for i in range(1,len(new_pred[series_num])):\n",
    "        prediction_tmp[i] = new_pred[series_num][i].pd_dataframe()\n",
    "    \n",
    "    res = imodwt(prediction_tmp.transpose().to_numpy(),'db2')\n",
    "    full_result.append(res)\n",
    "\n",
    "print(full_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd125fd3-6cdc-46d5-a44f-cc5941cc6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_result = TimeSeries.from_values(np.array(full_result))\n",
    "actual = TimeSeries.from_values(series[:,param_dset_lookback:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d931e1-f2b2-4666-88f8-7335aa930d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse(actual,full_result), rmse(actual,full_result),mae(actual,full_result), smape(actual,full_result), mape(actual,full_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swt_env",
   "language": "python",
   "name": "swt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
